This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.env.example
.gitignore
api.go
CLAUDE.md
config.go
errors.go
go.mod
IMPROVEMENTS.md
main.go
mcp_server.go
README.md
REVIEW.md
run_format.sh
run_lint.sh
run_test.sh
transport.go
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="api.go">
package main

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"time"
)

// CallAPI makes the actual API call - reusable for both CLI and MCP
func CallAPI(ctx context.Context, apiKey, baseURL, query, model, effort string, timeout time.Duration) (*apiResponse, error) {
	body := requestBody{
		Model: model,
		Input: query,
		Reasoning: reqReasoning{
			Effort: effort,
		},
		Tools: []reqTool{
			{Type: "web_search_preview"},
		},
	}

	buf, err := json.Marshal(body)
	if err != nil {
		return nil, fmt.Errorf("marshal request: %w", err)
	}

	req, err := http.NewRequestWithContext(ctx, http.MethodPost, baseURL, bytes.NewReader(buf))
	if err != nil {
		return nil, fmt.Errorf("build request: %w", err)
	}
	req.Header.Set("Content-Type", "application/json")
	req.Header.Set("Authorization", "Bearer "+apiKey)

	client := &http.Client{Timeout: timeout}
	resp, err := client.Do(req)
	if err != nil {
		return nil, fmt.Errorf("http request: %w", err)
	}
	defer resp.Body.Close()

	bodyBytes, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, fmt.Errorf("read response: %w", err)
	}

	if resp.StatusCode < 200 || resp.StatusCode >= 300 {
		return nil, &APIError{StatusCode: resp.StatusCode, Body: string(bodyBytes)}
	}

	var ar apiResponse
	if err := json.Unmarshal(bodyBytes, &ar); err != nil {
		return nil, fmt.Errorf("parse json: %w", err)
	}

	return &ar, nil
}

// ExtractAnswer extracts the answer text from the API response
func ExtractAnswer(apiResp *apiResponse) string {
	var answers []string
	for _, item := range apiResp.Output {
		if item.Type != "message" {
			continue
		}
		for _, content := range item.Content {
			if content.Type == "output_text" && content.Text != "" {
				answers = append(answers, content.Text)
			}
		}
	}

	// Join all text content into a single answer
	if len(answers) > 0 {
		// If multiple text segments, join them with space
		answer := ""
		for i, text := range answers {
			if i > 0 {
				answer += " "
			}
			answer += text
		}
		return answer
	}
	return ""
}

// HandleWebSearch handles web search requests for the MCP server
func HandleWebSearch(ctx context.Context, apiKey, baseURL string, args map[string]interface{}) (interface{}, error) {
	// Extract parameters
	query, ok := args["query"].(string)
	if !ok || query == "" {
		return map[string]interface{}{
			"success": false,
			"error":   "Please provide a query to search for",
		}, nil
	}

	model, _ := args["model"].(string) //nolint:errcheck // Type assertion ok to ignore
	if model == "" {
		model = defaultModel
	}

	effort, _ := args["reasoning_effort"].(string) //nolint:errcheck // Type assertion ok to ignore
	effort = validateEffort(effort)

	// Use effort-based timeout
	timeout := getTimeoutForEffort(effort)

	// Make API call
	apiResp, err := CallAPI(ctx, apiKey, baseURL, query, model, effort, timeout)
	if err != nil {
		return nil, err
	}

	// Extract the answer from response
	answer := ExtractAnswer(apiResp)
	if answer == "" {
		return map[string]interface{}{
			"success": false,
			"error":   "No answer found in response",
		}, nil
	}

	// Return structured response
	return map[string]interface{}{
		"success":      true,
		"answer":       answer,
		"query":        query,
		"model":        model,
		"effort":       effort,
		"timeout_used": timeout.String(),
	}, nil
}
</file>

<file path="IMPROVEMENTS.md">
# MCP Server Implementation Improvements

## Summary
This document outlines the improvements made to the Answer MCP server implementation to align with best practices and the latest `mcp-go` library API.

## Changes Made

### 1. Transport Layer Simplification (`transport.go`)

#### Before (327 lines)
- Unnecessarily complex implementation with goroutines, channels, and manual signal handling
- Mixed SSE transport logic with HTTP transport naming
- Manual HTTP server configuration and middleware
- Reinvented functionality that the library already provided

#### After (38 lines)
```go
// STDIO Transport - simplified to one line
return server.ServeStdio(mcpServer)

// HTTP Transport - using proper StreamableHTTP
httpServer := server.NewStreamableHTTPServer(mcpServer)
return httpServer.Start(addr)
```

#### Key Improvements:
- ✅ Removed unnecessary goroutines and channel orchestration
- ✅ Removed manual signal handling (library handles it)
- ✅ Used `server.ServeStdio()` directly instead of wrapper logic
- ✅ Used `StreamableHTTPServer` for proper HTTP transport
- ✅ Removed SSE transport (as requested)
- ✅ Removed custom middleware and HTML documentation
- ✅ Let the library handle all protocol details internally

### 2. MCP Server Configuration (`mcp_server.go`)

#### Fixed API Usage:
1. **Server Creation with Capabilities**
   ```go
   server.NewMCPServer(name, version,
       server.WithToolCapabilities(true),
       server.WithResourceCapabilities(true, false),
       server.WithPromptCapabilities(true),
   )
   ```

2. **Tool Definition Using Fluent API**
   - Changed from direct struct initialization to `mcp.NewTool()` builder pattern
   - Used proper helper methods: `mcp.Required()`, `mcp.DefaultString()`, `mcp.Enum()`
   - Proper argument extraction: `request.RequireString()`, `request.GetString()`

3. **Resource Using Fluent API**
   - Changed to `mcp.NewResource()` with proper URI format
   - Fixed return type (not pointer)

4. **Prompt Using Correct API**
   - Fixed from `mcp.WithPromptArgument` to `mcp.WithArgument`
   - Used `mcp.RequiredArgument()` and `mcp.ArgumentDescription()`
   - Fixed argument extraction (no type assertion needed)

5. **Separated System and User Messages**
   - Split combined prompt into proper system instructions and user message

## Benefits

### Code Quality
- **90% reduction in transport code** (327 → 38 lines)
- **Cleaner separation of concerns**
- **Better alignment with library design**
- **Reduced maintenance burden**

### Reliability
- **No custom lifecycle management** - library handles everything
- **No race conditions** from manual goroutines
- **Proper error handling** built into the library
- **Graceful shutdown** handled automatically

### Maintainability
- **Following documented patterns** from mcp-go examples
- **Using library APIs as intended**
- **Future-proof** against library updates
- **Clear and simple code**

## Lessons Learned

1. **Trust the library** - The `mcp-go` library provides simple, clean APIs that handle all complexity internally
2. **Don't reinvent the wheel** - Avoid wrapping library functions unnecessarily
3. **Use fluent APIs** - The builder pattern provides better type safety and cleaner code
4. **Keep it simple** - The simplest solution is often the correct one

## Testing

After improvements:
- ✅ Code compiles successfully
- ✅ STDIO transport starts correctly
- ✅ HTTP transport uses proper StreamableHTTP server
- ✅ All tools, resources, and prompts properly registered

## Next Steps

Consider these additional enhancements based on MCP documentation:
1. Add authentication middleware for HTTP transport
2. Implement rate limiting
3. Add more prompt templates for different use cases
4. Implement progress notifications for long-running searches
5. Add telemetry and metrics collection
</file>

<file path="REVIEW.md">
# MCP Server Implementation Review

## Analysis of Your MCP Server Implementation

Based on the MCP documentation and your code, here's my comprehensive review:

### ✅ **Strengths of Your Implementation**

1. **Dual Transport Support**: You've correctly implemented both STDIO and HTTP/SSE transports as recommended by the docs:
   - STDIO for Claude Desktop integration
   - HTTP/SSE for web-based clients

2. **Well-Structured Architecture**:
   - Clean separation of concerns (transport, server logic, API integration)
   - Dual-mode design (CLI + MCP server) is elegant
   - Good error handling with custom error types

3. **Tool Implementation**:
   - Your `gpt_websearch` tool follows MCP patterns correctly
   - Proper input schema with required/optional parameters
   - Good parameter validation

4. **Resource Implementation**:
   - Simple but effective `server-info` resource
   - Follows MCP resource patterns

### 🔧 **Areas for Enhancement Based on MCP Documentation**

1. **Prompt Implementation** - While functional, could be enhanced:
   ```go
   // Current: Single prompt with embedded instructions
   // Consider: Separate system vs user prompts for better structure
   
   // Enhanced version:
   messages := []mcp.PromptMessage{
       {
           Role: "system",
           Content: mcp.TextContent{
               Type: "text",
               Text: systemInstructions, // Move the tool usage instructions here
           },
       },
       {
           Role: "user", 
           Content: mcp.TextContent{
               Type: "text",
               Text: userQuestion,
           },
       },
   }
   ```

2. **HTTP Transport Enhancements**:
   Based on the HTTP transport docs, you could add:
   - Authentication middleware
   - Rate limiting
   - CORS headers for web clients
   - More standard MCP endpoints

3. **Prompt Templates**: The docs show advanced prompt patterns you could leverage:
   ```go
   // Add multiple prompt templates for different use cases
   mcpServer.AddPrompt(mcp.Prompt{
       Name: "quick_facts",
       Description: "Optimized for simple factual queries",
       Arguments: []mcp.PromptArgument{...},
   }, quickFactsHandler)
   
   mcpServer.AddPrompt(mcp.Prompt{
       Name: "deep_research", 
       Description: "For complex multi-part research",
       Arguments: []mcp.PromptArgument{...},
   }, deepResearchHandler)
   ```

4. **Session Management**: For the HTTP transport, consider adding:
   ```go
   // Stateless mode for better scalability
   sseServer := server.NewSSEServer(mcpServer,
       server.WithStateless(true),
   )
   ```

5. **Additional Resources**: Consider exposing more resources:
   ```go
   // Model capabilities resource
   mcpServer.AddResource(mcp.Resource{
       URI:         "models/capabilities",
       Name:        "Model Capabilities",
       Description: "Available models and their capabilities",
   }, modelsHandler)
   
   // Search history resource (if you track it)
   mcpServer.AddResource(mcp.Resource{
       URI:         "history/recent",
       Name:        "Recent Searches",
       Description: "Recent search queries and results",
   }, historyHandler)
   ```

### 📝 **Specific Suggestions**

1. **Enhanced Error Responses** for MCP:
   ```go
   // Instead of simple error strings, provide structured errors
   type MCPError struct {
       Code    string `json:"code"`
       Message string `json:"message"`
       Details map[string]interface{} `json:"details,omitempty"`
   }
   ```

2. **Tool Progress Notifications** (for long-running searches):
   ```go
   // Use MCP's notification system for progress updates
   mcpServer.SendNotification("progress", map[string]interface{}{
       "tool": "gpt_websearch",
       "status": "searching",
       "progress": 0.5,
   })
   ```

3. **HTTP Endpoint Documentation**:
   Your HTML documentation is good, but consider adding:
   - OpenAPI/Swagger spec
   - Example requests/responses
   - Authentication details (when added)

4. **Prompt Argument Validation**:
   ```go
   // Add more sophisticated argument validation
   Arguments: []mcp.PromptArgument{
       {
           Name:        "user_question",
           Description: "The question to answer",
           Required:    true,
           Schema: map[string]interface{}{
               "type": "string",
               "minLength": 1,
               "maxLength": 1000,
           },
       },
   }
   ```

### 🎯 **Key Recommendations**

1. **Leverage MCP Capabilities**: Your implementation is solid but conservative. The MCP framework supports more advanced features like:
   - Dynamic prompt generation based on context
   - Resource subscriptions for real-time updates
   - Tool composition (tools calling other tools)

2. **Consider Middleware Pattern** for HTTP:
   ```go
   // Add middleware chain for HTTP transport
   handler := middleware.Chain(
       middleware.Logger,
       middleware.RateLimit(100),
       middleware.Auth(apiKeyValidator),
   )(sseServer.SSEHandler())
   ```

3. **Add Telemetry/Metrics**:
   - Track tool usage
   - Monitor response times
   - Log error rates

### 📋 **MCP Documentation Insights**

#### Prompts (from https://mcp-go.dev/servers/prompts)
- **Prompt Fundamentals**: Reusable interaction templates for structuring LLM conversations
- **Key Components**: Support for required/optional arguments, defaults, constraints
- **Message Types**: Multi-message conversations with system/user/assistant roles
- **Advanced Patterns**: Embedded resources, conditional prompts, template-based prompts

#### HTTP Transport (from https://mcp-go.dev/transports/http)
- **StreamableHTTP Transport**: Provides REST-like interactions for MCP servers
- **Use Cases**: Microservices, public APIs, gateway integration, cached services
- **Standard Endpoints**: `/mcp/initialize`, `/mcp/tools/list`, `/mcp/tools/call`, `/mcp/resources/list`, `/mcp/health`
- **Configuration**: Custom endpoints, heartbeat intervals, stateless/stateful modes
- **Authentication**: JWT-based middleware examples provided

## Summary

Your implementation is well-structured and follows MCP patterns correctly. The main opportunities are in leveraging more advanced MCP features and adding production-ready concerns like authentication, rate limiting, and observability for the HTTP transport.

The code demonstrates a solid understanding of the MCP protocol and provides a clean, maintainable foundation for a web search service. The dual CLI/MCP mode is particularly elegant and provides good flexibility for different usage patterns.
</file>

<file path="run_format.sh">
#!/bin/sh

export PATH="/usr/local/go/bin:$PATH"

# Run gofmt to format all Go files recursively
/usr/local/go/bin/gofmt -w .
</file>

<file path="run_lint.sh">
#!/bin/sh

export PATH="/usr/local/go/bin:$PATH"
export HOME="/Users/rrj"
export GOLANGCI_LINT_CACHE="$HOME/Library/Caches/golangci-lint"
export GOCACHE="$HOME/.cache/go-build"

# Run linter
/Users/rrj/Projekty/Go/bin/golangci-lint run --fix ./...
</file>

<file path="run_test.sh">
#!/bin/sh

export PATH="/usr/local/go/bin:$PATH"

# Run tests
go test -v ./...
</file>

<file path=".env.example">
# This file contains environment variables for the application.
# Copy this file to .env and fill in the values.

# The question to ask the model.
QUESTION="What is the meaning of life?"

# The model to use (e.g., gpt-5-mini).
MODEL="gpt-5-mini"

# Show the full JSON response from the API.
SHOW_ALL=false

# The effort level for the model.
EFFORT="medium"

# The timeout for the API request (e.g., 30s, 1m).
TIMEOUT="300s"

# Your OpenAI API key.
OPENAI_API_KEY=""
</file>

<file path=".gitignore">
.DS_Store
bin/**
.env
go.sum
*.log

.aider*
.claude/**
.gemini/**
.vscode/**
.repomix/**

repomix.config.json

test_*
</file>

<file path="CLAUDE.md">
# CLAUDE.md

LLM guidance for working with Answer - dual-mode Go application (CLI + MCP server) for OpenAI web search.

## Build Commands
- `go build -o bin/answer .`
- `./run_format.sh` - format code 
- `./run_lint.sh` - lint (golangci-lint)
- `./run_test.sh` - run tests
- `./test_timeouts.sh` - timeout behavior testing

## Environment
Required: `OPENAI_API_KEY`
Optional: `MODEL`, `EFFORT`, `SHOW_ALL`, `TIMEOUT`, `QUESTION`
Uses godotenv for `.env` loading

## Architecture
**Dual-mode design**: CLI or MCP server based on args
- CLI: `./bin/answer "query"`
- MCP: `./bin/answer mcp [options]`

**Key files**:
- `main.go` - entry point, mode routing
- `mcp_server.go` - MCP server implementation (commit b6c3478 enhanced prompts)
- `api.go` - OpenAI API integration  
- `config.go` - environment config, timeouts
- `transport.go` - stdio/HTTP transports
- `errors.go` - error handling

## Recent Changes (commit b6c3478)
**Enhanced MCP Prompt System**: Replaced basic `web_search` with `intelligent_web_search`
- **Name**: `intelligent_web_search` (was `web_search`)
- **Argument**: `user_question` (was `topic`)
- **System Message**: Comprehensive LLM instructions for cost-effective tool usage

**Model Selection Logic**:
- `gpt-5-nano`: Simple facts, definitions, summaries
- `gpt-5-mini`: Research, comparisons, specific topics
- `gpt-5`: Complex analysis, coding, reasoning

**Reasoning Effort**:
- `low`: 3min timeout, factual queries
- `medium`: 5min timeout, synthesis tasks
- `high`: 10min timeout, complex analysis

**Search Strategy**: Single/sequential/parallel approaches based on query complexity

## MCP Implementation Details

**Tool**: `gpt_websearch` - web search using GPT models with:
- Model selection: gpt-5-nano/mini/full based on complexity
- Reasoning effort: low/medium/high with timeout mapping
- Query formulation: context-aware, detailed searches
- Strategy: single/sequential/parallel based on task

**Prompt Template**: `intelligent_web_search` (mcp_server.go:139-213) provides:
- Systematic LLM instructions for cost-effective tool usage
- Model selection guidelines by task complexity
- Search strategy optimization (single/sequential/parallel)
- Query formulation best practices

**Transports**: 
- STDIO: Claude Desktop integration
- HTTP/SSE: Web applications (port 8080)

## API Integration
- Endpoint: `https://api.openai.com/v1/responses`
- Tool type: `web_search_preview`
- Models: gpt-5, gpt-5-mini, gpt-5-nano
- Effort-based timeouts: 3/5/10 minutes

## Error Handling
- CLI: `fail()` function with exit codes (errors.go)
- MCP: Structured JSON responses
- API errors: Custom `APIError` type wrapping

## Configuration Priority
1. CLI flags
2. Environment variables  
3. Defaults (gpt-5-mini, low effort, 3min timeout)

## Testing
- `integration_test.go` - core API functions
- `test_timeouts.sh` - timeout behavior
- Environment-aware skipping for missing API keys

## Development Workflow
1. Set `OPENAI_API_KEY`
2. Make changes
3. `./run_format.sh && ./run_lint.sh && ./run_test.sh`
4. `go build -o bin/answer .`
5. Test CLI: `./bin/answer "query"`
6. Test MCP: `./bin/answer mcp -t stdio`

## Dependencies
- `github.com/mark3labs/mcp-go` v0.37.0 - MCP protocol
- `github.com/joho/godotenv` v1.5.1 - environment loading
</file>

<file path="config.go">
package main

import (
	"os"
	"strconv"
	"time"
)

const (
	// Default values
	defaultModel   = "gpt-5-mini"
	defaultEffort  = "low"
	defaultBaseURL = "https://api.openai.com/v1/responses"

	// Server metadata
	serverName    = "gpt-websearch-mcp"
	serverVersion = "1.0.0"

	// Timeouts based on reasoning effort
	timeoutLow    = 3 * time.Minute
	timeoutMedium = 5 * time.Minute
	timeoutHigh   = 10 * time.Minute
)

// API request/response structures
type reqReasoning struct {
	Effort string `json:"effort"`
}

type reqTool struct {
	Type string `json:"type"`
}

type requestBody struct {
	Model     string       `json:"model"`
	Input     string       `json:"input"`
	Reasoning reqReasoning `json:"reasoning"`
	Tools     []reqTool    `json:"tools"`
}

type respContent struct {
	Type string `json:"type"`
	Text string `json:"text"`
}

type respItem struct {
	Type    string        `json:"type"`
	Content []respContent `json:"content,omitempty"`
}

type apiResponse struct {
	Output []respItem `json:"output"`
}

// EnvConfig centralizes environment-derived configuration.
type EnvConfig struct {
	Question   string
	Model      string
	Effort     string
	ShowAll    bool
	HasShowAll bool
	Timeout    time.Duration
	HasTimeout bool
	APIKey     string
}

// loadEnvConfig reads environment variables
func loadEnvConfig() (EnvConfig, error) {
	cfg := EnvConfig{
		Question: os.Getenv("QUESTION"),
		Model:    os.Getenv("MODEL"),
		Effort:   os.Getenv("EFFORT"),
	}

	if v := os.Getenv("SHOW_ALL"); v != "" {
		if b, err := strconv.ParseBool(v); err == nil {
			cfg.ShowAll = b
			cfg.HasShowAll = true
		}
	}

	if v := os.Getenv("TIMEOUT"); v != "" {
		if d, err := time.ParseDuration(v); err == nil {
			cfg.Timeout = d
			cfg.HasTimeout = true
		}
	}

	cfg.APIKey = os.Getenv("OPENAI_API_KEY")
	if cfg.APIKey == "" {
		return EnvConfig{}, ErrNoAPIKey
	}

	return cfg, nil
}

// getTimeoutForEffort returns the appropriate timeout based on reasoning effort level
func getTimeoutForEffort(effort string) time.Duration {
	switch effort {
	case "high":
		return timeoutHigh
	case "medium":
		return timeoutMedium
	case "low", "":
		return timeoutLow
	default:
		return timeoutLow
	}
}

// validateEffort ensures the effort level is valid
func validateEffort(effort string) string {
	switch effort {
	case "low", "medium", "high":
		return effort
	case "":
		return defaultEffort
	default:
		return defaultEffort
	}
}
</file>

<file path="errors.go">
package main

import (
	"errors"
	"fmt"
	"os"
)

var (
	// Configuration errors
	ErrNoAPIKey = errors.New("OPENAI_API_KEY environment variable is required")

	// API errors
	ErrNoOutputText = errors.New("no output_text found in response")
	ErrAPIRequest   = errors.New("API request failed")

	// MCP errors
	ErrQueryRequired      = errors.New("please provide a query to search for")
	ErrInvalidEffort      = errors.New("invalid reasoning effort level")
	ErrSessionNotFound    = errors.New("session not found")
	ErrNotificationFailed = errors.New("failed to send notification")
)

// APIError represents an error from the OpenAI API
type APIError struct {
	StatusCode int
	Body       string
}

func (e *APIError) Error() string {
	return fmt.Sprintf("API error: status=%d body=%s", e.StatusCode, e.Body)
}

// fail prints to stderr and exits non-zero.
func fail(code int, msg string) {
	fmt.Fprintf(os.Stderr, "%s\n", msg)
	os.Exit(code)
}
</file>

<file path="README.md">
# Answer - GPT Web Search CLI & MCP Server

A Go application that provides intelligent web search capabilities using OpenAI's GPT models. Works as both a CLI tool and MCP (Model Context Protocol) server with cost-effective model selection.

## Features

- 🔍 **Intelligent Web Search**: Uses OpenAI's GPT models (gpt-5, gpt-5-mini, gpt-5-nano) with web search capabilities
- 🎯 **Cost-Effective**: Automatic model selection based on query complexity for optimal cost/performance
- 🚀 **Dual Mode**: CLI tool and MCP server with stdio/HTTP transports
- ⚙️ **Smart Configuration**: Effort-based timeouts (3/5/10 minutes) and environment-driven setup
- 🧠 **Enhanced MCP Prompts**: Intelligent prompt templates guide optimal tool usage
- 🔐 **Secure**: Environment-based API key management

## Installation

### Prerequisites

- Go 1.24.0 or later
- OpenAI API key with web search preview access

### Build from Source

```bash
# Clone the repository
git clone <repository-url>
cd Answer

# Install dependencies
go mod download

# Build the binary
go build -o bin/answer .

# Or install globally
go install .
```

## Configuration

### Environment Variables

Create a `.env` file in the project root:

```env
OPENAI_API_KEY=your-api-key-here
MODEL=gpt-5-mini         # Optional: gpt-5-mini (default), gpt-5, gpt-5-nano
EFFORT=low               # Optional: reasoning effort (low/medium/high)
SHOW_ALL=false           # Optional: show raw JSON
QUESTION=                # Optional: default question
```

**Model Selection Guidelines**:
- `gpt-5-nano`: Simple facts, definitions, quick lookups
- `gpt-5-mini`: Research tasks, comparisons, specific topics  
- `gpt-5`: Complex analysis, coding questions, reasoning tasks

**Effort-Based Timeouts**: `low` = 3 minutes, `medium` = 5 minutes, `high` = 10 minutes.

## Usage

### CLI Mode

Use Answer as a command-line tool for direct web searches:

```bash
# Simple query with positional argument
./bin/answer "Who won the 2024 Super Bowl?"

# Using the -q flag
./bin/answer -q "Latest AI developments"

# Error if no query provided
./bin/answer  # Error: please provide a question to ask

# With custom model and effort
./bin/answer -q "Explain quantum computing" -model gpt-5-mini -effort high

# Show raw JSON response
./bin/answer -q "Test query" -show-all

# Custom timeout
./bin/answer -q "Complex analysis" -timeout 120s
```

### MCP Server Mode

Run Answer as an MCP server for integration with AI assistants:

#### STDIO Transport (for Claude Desktop)

```bash
# Start MCP server in stdio mode (default)
./bin/answer mcp

# Or explicitly specify stdio
./bin/answer mcp -t stdio
./bin/answer mcp --transport stdio
```

**Claude Desktop Configuration:**

Add to `~/Library/Application Support/Claude/claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "gpt-websearch": {
      "command": "/path/to/Answer/bin/answer",
      "args": ["mcp", "-t", "stdio"],
      "env": {
        "OPENAI_API_KEY": "your-api-key"
      }
    }
  }
}
```

#### HTTP/SSE Transport (for Web Integration)

```bash
# Start HTTP server on default port 8080
./bin/answer mcp -t http

# Custom port
./bin/answer mcp -t http -port 3000

# With verbose logging
./bin/answer mcp -t http -verbose
```

**Endpoints:**
- `GET /` - API documentation
- `GET /health` - Health check
- `GET /sse` - Server-Sent Events for MCP protocol
- `GET /message` - Message handling endpoint

## MCP Server Features

### Tool: `gpt_websearch`
Performs intelligent web searches with cost-effective model selection:

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `query` | string | Yes | - | The search query or question |
| `model` | string | No | `gpt-5-mini` | GPT model: gpt-5-mini, gpt-5, or gpt-5-nano |
| `reasoning_effort` | string | No | `low` | Effort level:<br>`low` = 3 minutes<br>`medium` = 5 minutes<br>`high` = 10 minutes |

### Prompt: `web_search`
Enhanced prompt template that guides Claude Desktop to:
- Analyze user questions in conversation context
- Select cost-effective models based on complexity
- Choose appropriate reasoning effort levels
- Use single, sequential, or parallel search strategies

### Example Response

```json
{
  "success": true,
  "answer": "The complete answer to your query...",
  "query": "original query",
  "model": "gpt-5-mini",
  "effort": "low",
  "timeout_used": "3m0s"
}
```

## Command-Line Reference

### CLI Mode
```
answer [options] [question]

Options:
  -q, -question    Question to ask (required, can also use positional argument)
  -model          Model: gpt-5-mini (default), gpt-5, gpt-5-nano
  -effort         Reasoning effort: low (3min), medium (5min), high (10min timeout)
  -timeout        Request timeout (overrides effort-based defaults)
  -show-all       Show raw JSON response
  -base           API endpoint URL
```

### MCP Server Mode
```
answer mcp [options]

Options:
  -t, --transport  Transport type: stdio or http (default: stdio)
  -port           HTTP server port (default: 8080)
  -base           API endpoint URL
  -verbose        Enable verbose logging
```

## Examples

### CLI Examples

```bash
# Quick question
./bin/answer "What's the weather in San Francisco?"

# Research query with high effort
./bin/answer -q "Latest breakthroughs in quantum computing 2024" -effort high

# Using gpt-5-mini for research tasks
./bin/answer -q "Explain the theory of relativity" -model gpt-5-mini

# Debug mode with raw output
./bin/answer -q "Test query" -show-all
```

### MCP Integration Examples

**JavaScript SSE Client:**

```javascript
const eventSource = new EventSource('http://localhost:8080/sse');

eventSource.onmessage = (event) => {
  console.log('Received:', JSON.parse(event.data));
};

// Send request via fetch to message endpoint
fetch('http://localhost:8080/message', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    jsonrpc: '2.0',
    method: 'tools/call',
    params: {
      name: 'gpt_websearch',
      arguments: {
        query: 'Latest AI news',
        model: 'gpt-5-mini',
        reasoning_effort: 'medium'  // 5-minute timeout
      }
    },
    id: 1
  })
});
```

## Docker Support

```dockerfile
FROM golang:1.25-alpine AS builder
WORKDIR /app
COPY . .
RUN go mod download
RUN go build -o answer .

FROM alpine:latest
RUN apk --no-cache add ca-certificates
COPY --from=builder /app/answer /usr/local/bin/answer
ENV OPENAI_API_KEY=""
EXPOSE 8080
CMD ["answer", "mcp", "-t", "http"]
```

Build and run:
```bash
docker build -t answer .
docker run -p 8080:8080 -e OPENAI_API_KEY=your-key answer
```

## Development

### Project Structure
```
Answer/
├── main.go              # Main entry point with CLI and MCP modes
├── config.go           # Configuration structures and helpers
├── errors.go           # Error definitions
├── go.mod              # Go module definition
├── go.sum              # Dependency checksums
├── .env                # Environment variables (not committed)
├── bin/
│   └── answer         # Compiled binary
├── AGENTS.md          # Development guidelines
├── MCP_SERVER.md      # Detailed MCP documentation
└── README.md          # This file
```

### Building
```bash
go build -o bin/answer .
```

### Testing
```bash
go test ./...
```

### Formatting
```bash
go fmt ./...
```

## Troubleshooting

### Common Issues

1. **"OPENAI_API_KEY is not set"**
   - Set the environment variable or create a `.env` file

2. **"API error: status=401"**
   - Verify your API key is valid and has web search preview access

3. **"no output_text found in response"**
   - The model might not support web search
   - Try using a different model

4. **MCP client connection issues**
   - For stdio: Check that the binary path is correct
   - For HTTP: Verify the port is not in use

### Debug Mode

```bash
# CLI debug
./bin/answer -q "test" -show-all

# MCP debug
./bin/answer mcp -t stdio -verbose
./bin/answer mcp -t http -verbose
```

## License

See LICENSE file for details.

## Contributing

Please see [AGENTS.md](AGENTS.md) for development guidelines and contribution rules.

## Links

- [MCP Protocol Specification](https://modelcontextprotocol.io/)
- [OpenAI API Documentation](https://platform.openai.com/docs)
- [Detailed MCP Server Documentation](MCP_SERVER.md)
</file>

<file path="go.mod">
module Answer

go 1.25.0

require (
	github.com/joho/godotenv v1.5.1
	github.com/mark3labs/mcp-go v0.37.0
)

require (
	github.com/bahlo/generic-list-go v0.2.0 // indirect
	github.com/buger/jsonparser v1.1.1 // indirect
	github.com/google/uuid v1.6.0 // indirect
	github.com/invopop/jsonschema v0.13.0 // indirect
	github.com/mailru/easyjson v0.7.7 // indirect
	github.com/spf13/cast v1.7.1 // indirect
	github.com/wk8/go-ordered-map/v2 v2.1.8 // indirect
	github.com/yosida95/uritemplate/v3 v3.0.2 // indirect
	gopkg.in/yaml.v3 v3.0.1 // indirect
)
</file>

<file path="main.go">
package main

import (
	"context"
	"encoding/json"
	"flag"
	"fmt"
	"os"
	"time"

	_ "github.com/joho/godotenv/autoload"
)

func main() {
	// Check if this is MCP server mode
	if len(os.Args) > 1 && os.Args[1] == "mcp" {
		RunMCPServer()
		return
	}

	// Original CLI mode
	runCLI()
}

func runCLI() {
	// Load environment-backed configuration
	envCfg, err := loadEnvConfig()
	if err != nil {
		fail(2, err.Error())
	}

	// Parse CLI flags
	var (
		baseURL = flag.String("base", defaultBaseURL, "API endpoint")
		model   = flag.String("model", func() string {
			if envCfg.Model != "" {
				return envCfg.Model
			}
			return defaultModel
		}(), "model (env MODEL)")
		effort = flag.String("effort", func() string {
			if envCfg.Effort != "" {
				return envCfg.Effort
			}
			return defaultEffort
		}(), "effort (env EFFORT)")
		questionVal string
		timeout     = flag.Duration("timeout", func() time.Duration {
			if envCfg.HasTimeout {
				return envCfg.Timeout
			}
			return getTimeoutForEffort(*effort)
		}(), "HTTP timeout (env TIMEOUT)")
		showAll = flag.Bool("show-all", func() bool {
			if envCfg.HasShowAll {
				return envCfg.ShowAll
			}
			return false
		}(), "print raw JSON response (env SHOW_ALL)")
	)
	flag.StringVar(&questionVal, "q", envCfg.Question, "question prompt (env QUESTION)")
	flag.StringVar(&questionVal, "question", envCfg.Question, "same as -q (env QUESTION)")
	flag.Parse()

	// Determine the final question value
	q := questionVal
	var questionFlagSet bool
	flag.Visit(func(f *flag.Flag) {
		if f.Name == "q" || f.Name == "question" {
			questionFlagSet = true
		}
	})
	if !questionFlagSet {
		if flag.NArg() > 0 {
			q = flag.Arg(0)
		}
	}
	if q == "" {
		fail(2, "please provide a question to ask (use -q flag or positional argument)")
	}

	// If timeout wasn't explicitly set, use effort-based timeout
	if !envCfg.HasTimeout {
		flag.Visit(func(f *flag.Flag) {
			if f.Name == "timeout" {
				return // User set it explicitly
			}
		})
		*timeout = getTimeoutForEffort(*effort)
	}

	// Make API call
	ctx := context.Background()
	apiResp, err := CallAPI(ctx, envCfg.APIKey, *baseURL, q, *model, *effort, *timeout)
	if err != nil {
		fail(2, err.Error())
	}

	if *showAll {
		// Print the full raw JSON when requested
		raw, _ := json.MarshalIndent(apiResp, "", "  ") //nolint:errcheck // Debug output, error ok to ignore
		fmt.Println(string(raw))
		return
	}

	// Extract and print the answer
	answer := ExtractAnswer(apiResp)
	if answer == "" {
		fail(3, "no answer found in response")
	}
	fmt.Println(answer)
}
</file>

<file path="transport.go">
package main

import (
	"context"
	"fmt"
	"log"
	"os"

	"github.com/mark3labs/mcp-go/server"
)

// RunStdioTransport runs the MCP server using STDIO transport
func RunStdioTransport(ctx context.Context, mcpServer *server.MCPServer, sigChan chan os.Signal) error {
	// The server package provides a simple ServeStdio function
	// It handles all the complexity internally
	log.Println("Starting STDIO transport...")

	// Note: ServeStdio blocks and handles signals internally
	// The ctx and sigChan parameters are kept for interface compatibility
	// but the library manages its own lifecycle
	return server.ServeStdio(mcpServer)
}

// RunHTTPTransport runs the MCP server using StreamableHTTP transport
func RunHTTPTransport(ctx context.Context, mcpServer *server.MCPServer, port string, sigChan chan os.Signal) error {
	// Create StreamableHTTP server - this is the proper HTTP transport for MCP
	httpServer := server.NewStreamableHTTPServer(mcpServer)

	// Start the server on the specified port
	addr := fmt.Sprintf(":%s", port)
	log.Printf("Starting StreamableHTTP server on %s", addr)
	log.Printf("MCP endpoint: http://localhost:%s/", port)

	// Note: Start blocks and handles shutdown internally
	// The ctx and sigChan parameters are kept for interface compatibility
	// but the library manages its own lifecycle
	return httpServer.Start(addr)
}
</file>

<file path="mcp_server.go">
package main

import (
	"context"
	"encoding/json"
	"flag"
	"fmt"
	"log"
	"os"
	"os/signal"
	"syscall"

	"github.com/mark3labs/mcp-go/mcp"
	"github.com/mark3labs/mcp-go/server"
)

// RunMCPServer initializes and runs the MCP server
func RunMCPServer() {
	// Create a new flag set for MCP subcommand
	mcpFlags := flag.NewFlagSet("mcp", flag.ExitOnError)

	var (
		transport     = mcpFlags.String("t", "stdio", "Transport type")
		transportLong = mcpFlags.String("transport", "", "Transport type (overrides -t)")
		port          = mcpFlags.String("port", "8080", "HTTP server port")
		baseURL       = mcpFlags.String("base", defaultBaseURL, "API base URL")
		verbose       = mcpFlags.Bool("verbose", false, "Enable verbose logging")
	)

	// Parse MCP-specific flags (skip "answer mcp" args)
	mcpFlags.Parse(os.Args[2:]) //nolint:errcheck // Flag parsing error handling done by FlagSet

	// Use long form if provided
	if *transportLong != "" {
		*transport = *transportLong
	}

	// Configure logging
	if !*verbose {
		log.SetOutput(os.Stderr)
	}

	// Load environment config
	envCfg, err := loadEnvConfig()
	if err != nil {
		log.Fatalf("Failed to load config: %v", err)
	}

	// Create MCP server
	mcpServer := CreateMCPServer(envCfg.APIKey, *baseURL)

	// Setup context with cancellation
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	// Handle shutdown signals
	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, os.Interrupt, syscall.SIGTERM)

	switch *transport {
	case "stdio":
		if err := RunStdioTransport(ctx, mcpServer, sigChan); err != nil {
			log.Fatalf("STDIO transport error: %v", err)
		}
	case "http":
		if err := RunHTTPTransport(ctx, mcpServer, *port, sigChan); err != nil {
			log.Fatalf("HTTP transport error: %v", err)
		}
	default:
		log.Fatalf("Unknown transport: %s (use 'stdio' or 'http')", *transport)
	}
}

// CreateMCPServer creates and configures the MCP server with tools, resources, and prompts
func CreateMCPServer(apiKey, baseURL string) *server.MCPServer {
	// Create MCP server with capabilities
	mcpServer := server.NewMCPServer(
		serverName,
		serverVersion,
		server.WithToolCapabilities(true),
		server.WithResourceCapabilities(true, false),
		server.WithPromptCapabilities(true),
	)

	// Add web search tool using fluent API
	mcpServer.AddTool(
		mcp.NewTool("gpt_websearch",
			mcp.WithDescription("Search the web using OpenAI's GPT model with web search capabilities"),
			mcp.WithString("query",
				mcp.Required(),
				mcp.Description("The search query or question to ask"),
			),
			mcp.WithString("model",
				mcp.DefaultString(defaultModel),
				mcp.Description("The GPT model to use (default: gpt-5-mini)"),
			),
			mcp.WithString("reasoning_effort",
				mcp.DefaultString(defaultEffort),
				mcp.Description("Reasoning effort level: low (3min), medium (5min), or high (10min timeout)"),
				mcp.Enum("low", "medium", "high"),
			),
		),
		func(ctx context.Context, request mcp.CallToolRequest) (*mcp.CallToolResult, error) {
			// Use proper extraction methods
			query, err := request.RequireString("query")
			if err != nil {
				return mcp.NewToolResultError(err.Error()), nil
			}

			model := request.GetString("model", defaultModel)
			effort := request.GetString("reasoning_effort", defaultEffort)

			// Call handler with properly extracted values
			args := map[string]interface{}{
				"query":            query,
				"model":            model,
				"reasoning_effort": effort,
			}

			result, err := HandleWebSearch(ctx, apiKey, baseURL, args)
			if err != nil {
				return mcp.NewToolResultError(err.Error()), nil
			}

			// Convert result to JSON string for text content
			resultJSON, _ := json.Marshal(result) //nolint:errcheck // JSON marshal for simple types, error ok to ignore
			return mcp.NewToolResultText(string(resultJSON)), nil
		},
	)

	// Add server info resource using fluent API
	mcpServer.AddResource(
		mcp.NewResource(
			"server://info",
			"Server Information",
			mcp.WithResourceDescription("Information about the GPT Web Search MCP server"),
			mcp.WithMIMEType("text/plain"),
		),
		func(ctx context.Context, request mcp.ReadResourceRequest) ([]mcp.ResourceContents, error) {
			info := fmt.Sprintf("GPT Web Search MCP Server\nVersion: %s\nEndpoint: %s\n", serverVersion, baseURL)
			return []mcp.ResourceContents{
				mcp.TextResourceContents{
					URI:      request.Params.URI,
					MIMEType: "text/plain",
					Text:     info,
				},
			}, nil
		},
	)

	// Add web search prompt using fluent API
	mcpServer.AddPrompt(
		mcp.NewPrompt("intelligent_web_search",
			mcp.WithPromptDescription("Use the gpt_websearch tool to answer user questions based on web searching"),
			mcp.WithArgument("user_question",
				mcp.RequiredArgument(),
				mcp.ArgumentDescription("The question, task, problem, or instructions from the user that requires web search"),
			),
		),
		func(ctx context.Context, request mcp.GetPromptRequest) (*mcp.GetPromptResult, error) {
			userQuestion := request.Params.Arguments["user_question"]
			if userQuestion == "" {
				return nil, fmt.Errorf("user_question parameter is required")
			}

			// System instructions for using the web search tool
			systemPrompt := `You have access to the gpt_websearch tool that performs web searches using OpenAI's GPT models. This tool searches the web, gathers sources, reads them, and provides a single comprehensive answer.

CRITICAL: You MUST use the gpt_websearch tool to answer the user's question. Do not rely on your training data alone.

## Model Selection (choose cost-effectively):
- gpt-5-nano: Simple facts, definitions, quick lookups, basic summaries
- gpt-5-mini: Well-defined research tasks, comparisons, specific topics with clear scope
- gpt-5: Complex analysis, coding questions, multi-faceted problems, reasoning tasks

## Reasoning Effort Selection:
- low: Factual queries, simple definitions, straightforward questions (3 min timeout)
- medium: Research requiring synthesis, comparisons, moderate complexity (5 min timeout)
- high: Complex analysis, multi-part questions, deep research (10 min timeout)

## Search Strategy:
1. ANALYZE the user's question in the context of our conversation
2. FORMULATE detailed, specific search queries (expand beyond the original question with context and specifics)
3. DECIDE on search approach:
   - Single comprehensive search: When question can be fully addressed in one query
   - Sequential searches: When answers build on each other or need follow-up
   - Parallel searches: When covering different aspects of the same topic
4. SELECT appropriate model and reasoning_effort for each search
5. SYNTHESIZE results into a comprehensive, coherent answer

## Query Formulation Guidelines:
- Expand user questions with conversation context and specifics
- Include relevant constraints (timeframe, geographic scope, domain)
- Make queries specific enough to get focused, useful results
- Consider breaking complex questions into focused sub-queries

## Important Notes:
- The tool returns comprehensive answers, not citations or links to extract
- Be cost-conscious: use the simplest model that can handle the complexity
- You may need multiple searches for comprehensive coverage
- Always address the original user question completely

Now use the gpt_websearch tool strategically to answer the user's question.`

			// Return properly structured messages with system and user roles
			messages := []mcp.PromptMessage{
				{
					Role: "system",
					Content: mcp.TextContent{
						Type: "text",
						Text: systemPrompt,
					},
				},
				{
					Role: "user",
					Content: mcp.TextContent{
						Type: "text",
						Text: userQuestion,
					},
				},
			}

			return &mcp.GetPromptResult{
				Messages: messages,
			}, nil
		})

	return mcpServer
}
</file>

</files>
