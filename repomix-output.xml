This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.env.example
.gitignore
api.go
CLAUDE.md
config.go
errors.go
go.mod
IMPROVEMENTS.md
main.go
mcp_server.go
prompts.go
README.md
Release Notes.md
REVIEW.md
run_format.sh
run_lint.sh
run_test.sh
transport.go
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="IMPROVEMENTS.md">
# MCP Server Implementation Improvements

## Summary
This document outlines the improvements made to the Answer MCP server implementation to align with best practices and the latest `mcp-go` library API.

## Changes Made

### 1. Transport Layer Simplification (`transport.go`)

#### Before (327 lines)
- Unnecessarily complex implementation with goroutines, channels, and manual signal handling
- Mixed SSE transport logic with HTTP transport naming
- Manual HTTP server configuration and middleware
- Reinvented functionality that the library already provided

#### After (38 lines)
```go
// STDIO Transport - simplified to one line
return server.ServeStdio(mcpServer)

// HTTP Transport - using proper StreamableHTTP
httpServer := server.NewStreamableHTTPServer(mcpServer)
return httpServer.Start(addr)
```

#### Key Improvements:
- ✅ Removed unnecessary goroutines and channel orchestration
- ✅ Removed manual signal handling (library handles it)
- ✅ Used `server.ServeStdio()` directly instead of wrapper logic
- ✅ Used `StreamableHTTPServer` for proper HTTP transport
- ✅ Removed SSE transport (as requested)
- ✅ Removed custom middleware and HTML documentation
- ✅ Let the library handle all protocol details internally

### 2. MCP Server Configuration (`mcp_server.go`)

#### Fixed API Usage:
1. **Server Creation with Capabilities**
   ```go
   server.NewMCPServer(name, version,
       server.WithToolCapabilities(true),
       server.WithResourceCapabilities(true, false),
       server.WithPromptCapabilities(true),
   )
   ```

2. **Tool Definition Using Fluent API**
   - Changed from direct struct initialization to `mcp.NewTool()` builder pattern
   - Used proper helper methods: `mcp.Required()`, `mcp.DefaultString()`, `mcp.Enum()`
   - Proper argument extraction: `request.RequireString()`, `request.GetString()`

3. **Resource Using Fluent API**
   - Changed to `mcp.NewResource()` with proper URI format
   - Fixed return type (not pointer)

4. **Prompt Using Correct API**
   - Fixed from `mcp.WithPromptArgument` to `mcp.WithArgument`
   - Used `mcp.RequiredArgument()` and `mcp.ArgumentDescription()`
   - Fixed argument extraction (no type assertion needed)

5. **Separated System and User Messages**
   - Split combined prompt into proper system instructions and user message

## Benefits

### Code Quality
- **90% reduction in transport code** (327 → 38 lines)
- **Cleaner separation of concerns**
- **Better alignment with library design**
- **Reduced maintenance burden**

### Reliability
- **No custom lifecycle management** - library handles everything
- **No race conditions** from manual goroutines
- **Proper error handling** built into the library
- **Graceful shutdown** handled automatically

### Maintainability
- **Following documented patterns** from mcp-go examples
- **Using library APIs as intended**
- **Future-proof** against library updates
- **Clear and simple code**

## Lessons Learned

1. **Trust the library** - The `mcp-go` library provides simple, clean APIs that handle all complexity internally
2. **Don't reinvent the wheel** - Avoid wrapping library functions unnecessarily
3. **Use fluent APIs** - The builder pattern provides better type safety and cleaner code
4. **Keep it simple** - The simplest solution is often the correct one

## Testing

After improvements:
- ✅ Code compiles successfully
- ✅ STDIO transport starts correctly
- ✅ HTTP transport uses proper StreamableHTTP server
- ✅ All tools, resources, and prompts properly registered

## Next Steps

Consider these additional enhancements based on MCP documentation:
1. Add authentication middleware for HTTP transport
2. Implement rate limiting
3. Add more prompt templates for different use cases
4. Implement progress notifications for long-running searches
5. Add telemetry and metrics collection
</file>

<file path="prompts.go">
package main

// webSearchPrompt contains the MCP prompt for the gpt_websearch tool
const webSearchPrompt = `<context_gathering>
You have access to the gpt_websearch tool that performs web searches using OpenAI's GPT models. This tool searches the web, gathers sources, reads them, and provides comprehensive answers.

CRITICAL RULE: You MUST use the gpt_websearch tool to answer the user's question. Do not rely on your training data alone.
</context_gathering>

<parameter_optimization>
SELECT OPTIMAL PARAMETERS for cost-effectiveness and performance:

Model Selection:
- gpt-5-nano: Simple facts, definitions, quick lookups, basic summaries
- gpt-5-mini: Well-defined research tasks, comparisons, specific topics with clear scope  
- gpt-5: Complex analysis, coding questions, multi-faceted problems, reasoning tasks

Reasoning Effort Selection:
- minimal: Fastest time-to-first-token (90s timeout)
  USE FOR: Coding questions, instruction following, simple factual lookups, speed-critical tasks
- low: Quick responses for basic queries (3min timeout)
  USE FOR: Simple definitions, straightforward lookups without complex reasoning
- medium: Balanced reasoning for moderate complexity (5min timeout, DEFAULT)
  USE FOR: Research requiring synthesis, questions needing moderate analysis
- high: Deep analysis for complex tasks (10min timeout)
  USE FOR: Multi-faceted problems, comprehensive research, detailed investigations

Verbosity Selection:
- low: Concise responses with minimal commentary
  USE FOR: Quick facts, code-focused answers, situations requiring brevity
- medium: Balanced responses with moderate detail (DEFAULT)
  USE FOR: General-purpose queries, balanced explanations with reasonable depth
- high: Detailed responses with comprehensive explanations
  USE FOR: Learning scenarios, complex topics needing examples, thorough understanding

RECOMMENDED COMBINATIONS:
- Speed-Critical: gpt-5-nano + minimal + low
- Coding Questions: gpt-5 + minimal + medium/low
- Standard Research: gpt-5-mini + medium + medium  
- Complex Analysis: gpt-5 + high + high
- Learning/Educational: gpt-5-mini/gpt-5 + medium/high + high
</parameter_optimization>

<conversation_continuity>
PERFORMANCE-CRITICAL: GPT-5 reasoning models create internal reasoning chains. Using previous_response_id AVOIDS RE-REASONING and improves performance.

RULES:
1. ALWAYS capture the "id" field from each gpt_websearch response
2. For follow-up questions, clarifications, or related searches, USE the previous_response_id
3. This keeps interactions closer to the model's training distribution = BETTER PERFORMANCE

USE previous_response_id when:
- Following up on the same search results
- Asking for clarification or more detail on previous findings
- Building on previous research with related questions
- Requesting different formats/perspectives of the same information

DO NOT use previous_response_id for completely unrelated new topics.
</conversation_continuity>

<task_execution>
WORKFLOW for each user question:

1. ANALYZE: Determine if this relates to a previous search
   - If yes: USE previous_response_id to avoid re-reasoning
   - If no: Proceed with fresh search

2. PLAN: Select optimal model/effort/verbosity combination based on:
   - Question complexity
   - Response speed requirements  
   - Level of detail needed

3. FORMULATE: Create detailed, specific search queries
   - Expand beyond the original question with context and specifics
   - Include relevant constraints (timeframe, geographic scope, domain)
   - Make queries specific enough to get focused, useful results

4. EXECUTE: Perform search with optimal parameters
   - ALWAYS capture the response ID from results
   - For sequential searches, chain the response IDs to maintain reasoning continuity

5. SYNTHESIZE: Provide comprehensive, coherent answer addressing the original question completely
</task_execution>

<persistence>
Continue working until the user's query is completely resolved. You may need multiple searches for comprehensive coverage. Do not ask for confirmation - make reasonable assumptions and proceed with follow-up searches if needed to fully address the question.

For multi-search strategies:
- Chain response IDs between related searches
- Use previous_response_id when expanding on or clarifying previous results
- Remember: Better performance comes from avoiding duplicate reasoning through proper ID usage
</persistence>

<final_instructions>
The gpt_websearch tool returns comprehensive answers, not citations or links to extract. Be cost-conscious by using the simplest model that can handle the complexity, but ensure you fully address the user's question.

Now analyze the user's question and use the gpt_websearch tool strategically with optimal parameters.
</final_instructions>`
</file>

<file path="Release Notes.md">
# Release Notes

## v0.2.0 - 2025-08-18

### 🎉 New Features
- **Conversation Continuity**: Implemented conversation continuity for web searches, allowing for more context-aware interactions.
- **Web Search Tool**: Added a web search tool and server functionality to the API.
- **Logging**: Enabled and integrated client logging for better debugging and monitoring.
- **Configuration**: The application now loads configuration from environment variables, providing more flexibility.
- **CLI**: Added a Go CLI for interacting with the API.
- **Default Model**: Set `gpt-5-mini` as the default model for all interactions.

### 🔧 Improvements
- **Prompt Engineering**: Refined and enhanced web search prompts for clarity, efficiency, and cost-effectiveness.
- **Code Refactoring**: Simplified web search prompt construction and refactored the MCP server to use new library features, improving code quality and maintainability.

### 📚 Documentation
- Added and updated release notes.
</file>

<file path="REVIEW.md">
# MCP Server Implementation Review

## Analysis of Your MCP Server Implementation

Based on the MCP documentation and your code, here's my comprehensive review:

### ✅ **Strengths of Your Implementation**

1. **Dual Transport Support**: You've correctly implemented both STDIO and HTTP/SSE transports as recommended by the docs:
   - STDIO for Claude Desktop integration
   - HTTP/SSE for web-based clients

2. **Well-Structured Architecture**:
   - Clean separation of concerns (transport, server logic, API integration)
   - Dual-mode design (CLI + MCP server) is elegant
   - Good error handling with custom error types

3. **Tool Implementation**:
   - Your `gpt_websearch` tool follows MCP patterns correctly
   - Proper input schema with required/optional parameters
   - Good parameter validation

4. **Resource Implementation**:
   - Simple but effective `server-info` resource
   - Follows MCP resource patterns

### 🔧 **Areas for Enhancement Based on MCP Documentation**

1. **Prompt Implementation** - While functional, could be enhanced:
   ```go
   // Current: Single prompt with embedded instructions
   // Consider: Separate system vs user prompts for better structure
   
   // Enhanced version:
   messages := []mcp.PromptMessage{
       {
           Role: "system",
           Content: mcp.TextContent{
               Type: "text",
               Text: systemInstructions, // Move the tool usage instructions here
           },
       },
       {
           Role: "user", 
           Content: mcp.TextContent{
               Type: "text",
               Text: userQuestion,
           },
       },
   }
   ```

2. **HTTP Transport Enhancements**:
   Based on the HTTP transport docs, you could add:
   - Authentication middleware
   - Rate limiting
   - CORS headers for web clients
   - More standard MCP endpoints

3. **Prompt Templates**: The docs show advanced prompt patterns you could leverage:
   ```go
   // Add multiple prompt templates for different use cases
   mcpServer.AddPrompt(mcp.Prompt{
       Name: "quick_facts",
       Description: "Optimized for simple factual queries",
       Arguments: []mcp.PromptArgument{...},
   }, quickFactsHandler)
   
   mcpServer.AddPrompt(mcp.Prompt{
       Name: "deep_research", 
       Description: "For complex multi-part research",
       Arguments: []mcp.PromptArgument{...},
   }, deepResearchHandler)
   ```

4. **Session Management**: For the HTTP transport, consider adding:
   ```go
   // Stateless mode for better scalability
   sseServer := server.NewSSEServer(mcpServer,
       server.WithStateless(true),
   )
   ```

5. **Additional Resources**: Consider exposing more resources:
   ```go
   // Model capabilities resource
   mcpServer.AddResource(mcp.Resource{
       URI:         "models/capabilities",
       Name:        "Model Capabilities",
       Description: "Available models and their capabilities",
   }, modelsHandler)
   
   // Search history resource (if you track it)
   mcpServer.AddResource(mcp.Resource{
       URI:         "history/recent",
       Name:        "Recent Searches",
       Description: "Recent search queries and results",
   }, historyHandler)
   ```

### 📝 **Specific Suggestions**

1. **Enhanced Error Responses** for MCP:
   ```go
   // Instead of simple error strings, provide structured errors
   type MCPError struct {
       Code    string `json:"code"`
       Message string `json:"message"`
       Details map[string]interface{} `json:"details,omitempty"`
   }
   ```

2. **Tool Progress Notifications** (for long-running searches):
   ```go
   // Use MCP's notification system for progress updates
   mcpServer.SendNotification("progress", map[string]interface{}{
       "tool": "gpt_websearch",
       "status": "searching",
       "progress": 0.5,
   })
   ```

3. **HTTP Endpoint Documentation**:
   Your HTML documentation is good, but consider adding:
   - OpenAPI/Swagger spec
   - Example requests/responses
   - Authentication details (when added)

4. **Prompt Argument Validation**:
   ```go
   // Add more sophisticated argument validation
   Arguments: []mcp.PromptArgument{
       {
           Name:        "user_question",
           Description: "The question to answer",
           Required:    true,
           Schema: map[string]interface{}{
               "type": "string",
               "minLength": 1,
               "maxLength": 1000,
           },
       },
   }
   ```

### 🎯 **Key Recommendations**

1. **Leverage MCP Capabilities**: Your implementation is solid but conservative. The MCP framework supports more advanced features like:
   - Dynamic prompt generation based on context
   - Resource subscriptions for real-time updates
   - Tool composition (tools calling other tools)

2. **Consider Middleware Pattern** for HTTP:
   ```go
   // Add middleware chain for HTTP transport
   handler := middleware.Chain(
       middleware.Logger,
       middleware.RateLimit(100),
       middleware.Auth(apiKeyValidator),
   )(sseServer.SSEHandler())
   ```

3. **Add Telemetry/Metrics**:
   - Track tool usage
   - Monitor response times
   - Log error rates

### 📋 **MCP Documentation Insights**

#### Prompts (from https://mcp-go.dev/servers/prompts)
- **Prompt Fundamentals**: Reusable interaction templates for structuring LLM conversations
- **Key Components**: Support for required/optional arguments, defaults, constraints
- **Message Types**: Multi-message conversations with system/user/assistant roles
- **Advanced Patterns**: Embedded resources, conditional prompts, template-based prompts

#### HTTP Transport (from https://mcp-go.dev/transports/http)
- **StreamableHTTP Transport**: Provides REST-like interactions for MCP servers
- **Use Cases**: Microservices, public APIs, gateway integration, cached services
- **Standard Endpoints**: `/mcp/initialize`, `/mcp/tools/list`, `/mcp/tools/call`, `/mcp/resources/list`, `/mcp/health`
- **Configuration**: Custom endpoints, heartbeat intervals, stateless/stateful modes
- **Authentication**: JWT-based middleware examples provided

## Summary

Your implementation is well-structured and follows MCP patterns correctly. The main opportunities are in leveraging more advanced MCP features and adding production-ready concerns like authentication, rate limiting, and observability for the HTTP transport.

The code demonstrates a solid understanding of the MCP protocol and provides a clean, maintainable foundation for a web search service. The dual CLI/MCP mode is particularly elegant and provides good flexibility for different usage patterns.
</file>

<file path="run_format.sh">
#!/bin/sh

export PATH="/usr/local/go/bin:$PATH"

# Run gofmt to format all Go files recursively
/usr/local/go/bin/gofmt -w .
</file>

<file path="run_lint.sh">
#!/bin/sh

export PATH="/usr/local/go/bin:$PATH"
export HOME="/Users/rrj"
export GOLANGCI_LINT_CACHE="$HOME/Library/Caches/golangci-lint"
export GOCACHE="$HOME/.cache/go-build"

# Run linter
/Users/rrj/Projekty/Go/bin/golangci-lint run --fix ./...
</file>

<file path="run_test.sh">
#!/bin/sh

export PATH="/usr/local/go/bin:$PATH"

# Run tests
go test -v ./...
</file>

<file path=".env.example">
# This file contains environment variables for the application.
# Copy this file to .env and fill in the values.

# The question to ask the model.
QUESTION="What is the meaning of life?"

# The model to use (e.g., gpt-5-mini).
MODEL="gpt-5-mini"

# Show the full JSON response from the API.
SHOW_ALL=false

# The effort level for the model.
EFFORT="medium"

# The timeout for the API request (e.g., 30s, 1m).
TIMEOUT="300s"

# Your OpenAI API key.
OPENAI_API_KEY=""
</file>

<file path=".gitignore">
.DS_Store
bin/**
.env
go.sum
*.log

.aider*
.claude/**
.gemini/**
.vscode/**
.repomix/**

repomix.config.json

test_*
</file>

<file path="CLAUDE.md">
# CLAUDE.md

LLM guidance for working with Answer - dual-mode Go application (CLI + MCP server) for OpenAI web search.

## Build Commands
- `go build -o bin/answer .`
- `./run_format.sh` - format code 
- `./run_lint.sh` - lint (golangci-lint)
- `./run_test.sh` - run tests
- `./test_timeouts.sh` - timeout behavior testing

## Environment
Required: `OPENAI_API_KEY`
Optional: `MODEL`, `EFFORT`, `SHOW_ALL`, `TIMEOUT`, `QUESTION`
Uses godotenv for `.env` loading

## Architecture
**Dual-mode design**: CLI or MCP server based on args
- CLI: `./bin/answer "query"`
- MCP: `./bin/answer mcp [options]`

**Key files**:
- `main.go` - entry point, mode routing
- `mcp_server.go` - MCP server implementation (commit b6c3478 enhanced prompts)
- `api.go` - OpenAI API integration  
- `config.go` - environment config, timeouts
- `transport.go` - stdio/HTTP transports
- `errors.go` - error handling

## Recent Changes (commit b6c3478)
**Enhanced MCP Prompt System**: Replaced basic `web_search` with `intelligent_web_search`
- **Name**: `intelligent_web_search` (was `web_search`)
- **Argument**: `user_question` (was `topic`)
- **System Message**: Comprehensive LLM instructions for cost-effective tool usage

**Model Selection Logic**:
- `gpt-5-nano`: Simple facts, definitions, summaries
- `gpt-5-mini`: Research, comparisons, specific topics
- `gpt-5`: Complex analysis, coding, reasoning

**Reasoning Effort**:
- `low`: 3min timeout, factual queries
- `medium`: 5min timeout, synthesis tasks
- `high`: 10min timeout, complex analysis

**Search Strategy**: Single/sequential/parallel approaches based on query complexity

## MCP Implementation Details

**Tool**: `gpt_websearch` - web search using GPT models with:
- Model selection: gpt-5-nano/mini/full based on complexity
- Reasoning effort: low/medium/high with timeout mapping
- Query formulation: context-aware, detailed searches
- Strategy: single/sequential/parallel based on task

**Prompt Template**: `intelligent_web_search` (mcp_server.go:139-213) provides:
- Systematic LLM instructions for cost-effective tool usage
- Model selection guidelines by task complexity
- Search strategy optimization (single/sequential/parallel)
- Query formulation best practices

**Transports**: 
- STDIO: Claude Desktop integration
- HTTP/SSE: Web applications (port 8080)

## API Integration
- Endpoint: `https://api.openai.com/v1/responses`
- Tool type: `web_search_preview`
- Models: gpt-5, gpt-5-mini, gpt-5-nano
- Effort-based timeouts: 3/5/10 minutes

## Error Handling
- CLI: `fail()` function with exit codes (errors.go)
- MCP: Structured JSON responses
- API errors: Custom `APIError` type wrapping

## Configuration Priority
1. CLI flags
2. Environment variables  
3. Defaults (gpt-5-mini, low effort, 3min timeout)

## Testing
- `integration_test.go` - core API functions
- `test_timeouts.sh` - timeout behavior
- Environment-aware skipping for missing API keys

## Development Workflow
1. Set `OPENAI_API_KEY`
2. Make changes
3. `./run_format.sh && ./run_lint.sh && ./run_test.sh`
4. `go build -o bin/answer .`
5. Test CLI: `./bin/answer "query"`
6. Test MCP: `./bin/answer mcp -t stdio`

## Dependencies
- `github.com/mark3labs/mcp-go` v0.37.0 - MCP protocol
- `github.com/joho/godotenv` v1.5.1 - environment loading
</file>

<file path="errors.go">
package main

import (
	"errors"
	"fmt"
	"os"
)

var (
	// Configuration errors
	ErrNoAPIKey = errors.New("OPENAI_API_KEY environment variable is required")

	// API errors
	ErrNoOutputText = errors.New("no output_text found in response")
	ErrAPIRequest   = errors.New("API request failed")

	// MCP errors
	ErrQueryRequired      = errors.New("please provide a query to search for")
	ErrInvalidEffort      = errors.New("invalid reasoning effort level")
	ErrSessionNotFound    = errors.New("session not found")
	ErrNotificationFailed = errors.New("failed to send notification")
)

// APIError represents an error from the OpenAI API
type APIError struct {
	StatusCode int
	Body       string
}

func (e *APIError) Error() string {
	return fmt.Sprintf("API error: status=%d body=%s", e.StatusCode, e.Body)
}

// fail prints to stderr and exits non-zero.
func fail(code int, msg string) {
	fmt.Fprintf(os.Stderr, "%s\n", msg)
	os.Exit(code)
}
</file>

<file path="go.mod">
module Answer

go 1.25.0

require (
	github.com/joho/godotenv v1.5.1
	github.com/mark3labs/mcp-go v0.37.0
)

require (
	github.com/bahlo/generic-list-go v0.2.0 // indirect
	github.com/buger/jsonparser v1.1.1 // indirect
	github.com/google/uuid v1.6.0 // indirect
	github.com/invopop/jsonschema v0.13.0 // indirect
	github.com/mailru/easyjson v0.7.7 // indirect
	github.com/spf13/cast v1.7.1 // indirect
	github.com/wk8/go-ordered-map/v2 v2.1.8 // indirect
	github.com/yosida95/uritemplate/v3 v3.0.2 // indirect
	gopkg.in/yaml.v3 v3.0.1 // indirect
)
</file>

<file path="README.md">
# Answer - GPT Web Search CLI & MCP Server

A Go application that provides intelligent web search capabilities using OpenAI's GPT models. Works as both a CLI tool and MCP (Model Context Protocol) server with cost-effective model selection.

## Features

- 🔍 **Intelligent Web Search**: Uses OpenAI's GPT models (gpt-5, gpt-5-mini, gpt-5-nano) with web search capabilities
- 🎯 **Cost-Effective**: Automatic model selection based on query complexity for optimal cost/performance
- 🚀 **Dual Mode**: CLI tool and MCP server with stdio/HTTP transports
- ⚙️ **Smart Configuration**: Effort-based timeouts (3/5/10 minutes) and environment-driven setup
- 🧠 **Enhanced MCP Prompts**: Intelligent prompt templates guide optimal tool usage
- 🔄 **Conversation Continuity**: Response IDs enable follow-up questions with maintained context
- 🔐 **Secure**: Environment-based API key management

## Installation

### Prerequisites

- Go 1.24.0 or later
- OpenAI API key with web search preview access

### Build from Source

```bash
# Clone the repository
git clone <repository-url>
cd Answer

# Install dependencies
go mod download

# Build the binary
go build -o bin/answer .

# Or install globally
go install .
```

## Configuration

### Environment Variables

Create a `.env` file in the project root:

```env
OPENAI_API_KEY=your-api-key-here
MODEL=gpt-5-mini         # Optional: gpt-5-mini (default), gpt-5, gpt-5-nano
EFFORT=low               # Optional: reasoning effort (low/medium/high)
SHOW_ALL=false           # Optional: show raw JSON
QUESTION=                # Optional: default question
```

**Model Selection Guidelines**:
- `gpt-5-nano`: Simple facts, definitions, quick lookups
- `gpt-5-mini`: Research tasks, comparisons, specific topics  
- `gpt-5`: Complex analysis, coding questions, reasoning tasks

**Effort-Based Timeouts**: `low` = 3 minutes, `medium` = 5 minutes, `high` = 10 minutes.

## Usage

### CLI Mode

Use Answer as a command-line tool for direct web searches:

```bash
# Simple query with positional argument
./bin/answer "Who won the 2024 Super Bowl?"

# Using the -q flag
./bin/answer -q "Latest AI developments"

# Error if no query provided
./bin/answer  # Error: please provide a question to ask

# With custom model and effort
./bin/answer -q "Explain quantum computing" -model gpt-5-mini -effort high

# Show raw JSON response
./bin/answer -q "Test query" -show-all

# Custom timeout
./bin/answer -q "Complex analysis" -timeout 120s
```

### MCP Server Mode

Run Answer as an MCP server for integration with AI assistants:

#### STDIO Transport (for Claude Desktop)

```bash
# Start MCP server in stdio mode (default)
./bin/answer mcp

# Or explicitly specify stdio
./bin/answer mcp -t stdio
./bin/answer mcp --transport stdio
```

**Claude Desktop Configuration:**

Add to `~/Library/Application Support/Claude/claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "gpt-websearch": {
      "command": "/path/to/Answer/bin/answer",
      "args": ["mcp", "-t", "stdio"],
      "env": {
        "OPENAI_API_KEY": "your-api-key"
      }
    }
  }
}
```

#### HTTP/SSE Transport (for Web Integration)

```bash
# Start HTTP server on default port 8080
./bin/answer mcp -t http

# Custom port
./bin/answer mcp -t http -port 3000

# With verbose logging
./bin/answer mcp -t http -verbose
```

**Endpoints:**
- `GET /` - API documentation
- `GET /health` - Health check
- `GET /sse` - Server-Sent Events for MCP protocol
- `GET /message` - Message handling endpoint

## MCP Server Features

### Tool: `gpt_websearch`
Performs intelligent web searches with cost-effective model selection and conversation continuity:

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `query` | string | Yes | - | The search query or question |
| `model` | string | No | `gpt-5-mini` | GPT model: gpt-5-mini, gpt-5, or gpt-5-nano |
| `reasoning_effort` | string | No | `low` | Effort level:<br>`low` = 3 minutes<br>`medium` = 5 minutes<br>`high` = 10 minutes |
| `previous_response_id` | string | No | - | Previous response ID for conversation continuity |

### Prompt: `web_search`
Enhanced prompt template that guides Claude Desktop to:
- Analyze user questions in conversation context
- Select cost-effective models based on complexity
- Choose appropriate reasoning effort levels
- Use single, sequential, or parallel search strategies
- Remember and use response IDs for conversation continuity

### Example Response

```json
{
  "success": true,
  "answer": "The complete answer to your query...",
  "query": "original query",
  "model": "gpt-5-mini-2025-08-07",
  "effort": "low",
  "timeout_used": "3m0s",
  "id": "resp_68a24ac476a081a09c4c914ee8827c2b0f42d84e6960dd2d",
  "requested_model": "gpt-5-mini",
  "requested_effort": "low"
}
```

### Conversation Continuity

The MCP server supports conversation continuity through response IDs. Each search response includes an `id` field that can be used in follow-up queries to maintain context:

**Initial Query:**
```json
{
  "name": "gpt_websearch",
  "arguments": {
    "query": "Tell me about Luxembourg City",
    "model": "gpt-5-mini",
    "reasoning_effort": "medium"
  }
}
```

**Response includes ID:**
```json
{
  "id": "resp_68a24ac476a081a09c4c914ee8827c2b0f42d84e6960dd2d",
  "answer": "Luxembourg City is the capital...",
  // ... other fields
}
```

**Follow-up Query with Context:**
```json
{
  "name": "gpt_websearch",
  "arguments": {
    "query": "What are the main tourist attractions there?",
    "previous_response_id": "resp_68a24ac476a081a09c4c914ee8827c2b0f42d84e6960dd2d",
    "reasoning_effort": "low"
  }
}
```

The AI assistant will automatically remember context from the previous search and provide more relevant answers for follow-up questions.

## Command-Line Reference

### CLI Mode
```
answer [options] [question]

Options:
  -q, -question    Question to ask (required, can also use positional argument)
  -model          Model: gpt-5-mini (default), gpt-5, gpt-5-nano
  -effort         Reasoning effort: low (3min), medium (5min), high (10min timeout)
  -timeout        Request timeout (overrides effort-based defaults)
  -show-all       Show raw JSON response
  -base           API endpoint URL
```

### MCP Server Mode
```
answer mcp [options]

Options:
  -t, --transport  Transport type: stdio or http (default: stdio)
  -port           HTTP server port (default: 8080)
  -base           API endpoint URL
  -verbose        Enable verbose logging
```

## Examples

### CLI Examples

```bash
# Quick question
./bin/answer "What's the weather in San Francisco?"

# Research query with high effort
./bin/answer -q "Latest breakthroughs in quantum computing 2024" -effort high

# Using gpt-5-mini for research tasks
./bin/answer -q "Explain the theory of relativity" -model gpt-5-mini

# Debug mode with raw output
./bin/answer -q "Test query" -show-all
```

### MCP Integration Examples

**JavaScript SSE Client:**

```javascript
const eventSource = new EventSource('http://localhost:8080/sse');

eventSource.onmessage = (event) => {
  console.log('Received:', JSON.parse(event.data));
};

// Send request via fetch to message endpoint
fetch('http://localhost:8080/message', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    jsonrpc: '2.0',
    method: 'tools/call',
    params: {
      name: 'gpt_websearch',
      arguments: {
        query: 'Latest AI news',
        model: 'gpt-5-mini',
        reasoning_effort: 'medium',  // 5-minute timeout
        previous_response_id: 'resp_123...'  // Optional: for follow-up questions
      }
    },
    id: 1
  })
});
```

## Docker Support

```dockerfile
FROM golang:1.25-alpine AS builder
WORKDIR /app
COPY . .
RUN go mod download
RUN go build -o answer .

FROM alpine:latest
RUN apk --no-cache add ca-certificates
COPY --from=builder /app/answer /usr/local/bin/answer
ENV OPENAI_API_KEY=""
EXPOSE 8080
CMD ["answer", "mcp", "-t", "http"]
```

Build and run:
```bash
docker build -t answer .
docker run -p 8080:8080 -e OPENAI_API_KEY=your-key answer
```

## Development

### Project Structure
```
Answer/
├── main.go              # Main entry point with CLI and MCP modes
├── config.go           # Configuration structures and helpers
├── errors.go           # Error definitions
├── go.mod              # Go module definition
├── go.sum              # Dependency checksums
├── .env                # Environment variables (not committed)
├── bin/
│   └── answer         # Compiled binary
├── AGENTS.md          # Development guidelines
├── MCP_SERVER.md      # Detailed MCP documentation
└── README.md          # This file
```

### Building
```bash
go build -o bin/answer .
```

### Testing
```bash
go test ./...
```

### Formatting
```bash
go fmt ./...
```

## Troubleshooting

### Common Issues

1. **"OPENAI_API_KEY is not set"**
   - Set the environment variable or create a `.env` file

2. **"API error: status=401"**
   - Verify your API key is valid and has web search preview access

3. **"no output_text found in response"**
   - The model might not support web search
   - Try using a different model

4. **MCP client connection issues**
   - For stdio: Check that the binary path is correct
   - For HTTP: Verify the port is not in use

### Debug Mode

```bash
# CLI debug
./bin/answer -q "test" -show-all

# MCP debug
./bin/answer mcp -t stdio -verbose
./bin/answer mcp -t http -verbose
```

## License

See LICENSE file for details.

## Contributing

Please see [AGENTS.md](AGENTS.md) for development guidelines and contribution rules.

## Links

- [MCP Protocol Specification](https://modelcontextprotocol.io/)
- [OpenAI API Documentation](https://platform.openai.com/docs)
- [Detailed MCP Server Documentation](MCP_SERVER.md)
</file>

<file path="api.go">
package main

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"time"

	"github.com/mark3labs/mcp-go/mcp"
	"github.com/mark3labs/mcp-go/server"
)

// CallAPI makes the actual API call - reusable for both CLI and MCP
func CallAPI(ctx context.Context, apiKey, baseURL, query, model, effort, verbosity, previousResponseID string, timeout time.Duration) (*apiResponse, error) {
	body := requestBody{
		Model: model,
		Input: query,
		Reasoning: reqReasoning{
			Effort: effort,
		},
		Text: reqText{
			Verbosity: verbosity,
		},
		Tools: []reqTool{
			{Type: "web_search_preview"},
		},
		PreviousResponseID: previousResponseID,
	}

	buf, err := json.Marshal(body)
	if err != nil {
		return nil, fmt.Errorf("marshal request: %w", err)
	}

	req, err := http.NewRequestWithContext(ctx, http.MethodPost, baseURL, bytes.NewReader(buf))
	if err != nil {
		return nil, fmt.Errorf("build request: %w", err)
	}
	req.Header.Set("Content-Type", "application/json")
	req.Header.Set("Authorization", "Bearer "+apiKey)

	client := &http.Client{Timeout: timeout}
	resp, err := client.Do(req)
	if err != nil {
		return nil, fmt.Errorf("http request: %w", err)
	}
	defer resp.Body.Close()

	bodyBytes, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, fmt.Errorf("read response: %w", err)
	}

	if resp.StatusCode < 200 || resp.StatusCode >= 300 {
		return nil, &APIError{StatusCode: resp.StatusCode, Body: string(bodyBytes)}
	}

	var ar apiResponse
	if err := json.Unmarshal(bodyBytes, &ar); err != nil {
		return nil, fmt.Errorf("parse json: %w", err)
	}

	return &ar, nil
}

// ExtractAnswer extracts the answer text from the API response
func ExtractAnswer(apiResp *apiResponse) string {
	var answers []string
	for _, item := range apiResp.Output {
		if item.Type != "message" {
			continue
		}
		for _, content := range item.Content {
			if content.Type == "output_text" && content.Text != "" {
				answers = append(answers, content.Text)
			}
		}
	}

	// Join all text content into a single answer
	if len(answers) > 0 {
		// If multiple text segments, join them with space
		answer := ""
		for i, text := range answers {
			if i > 0 {
				answer += " "
			}
			answer += text
		}
		return answer
	}
	return ""
}

// HandleWebSearch handles web search requests for the MCP server
func HandleWebSearch(ctx context.Context, apiKey, baseURL string, args map[string]interface{}) (interface{}, error) {
	// Get the MCP server from context for logging (if available)
	mcpServer := server.ServerFromContext(ctx)

	// Extract parameters
	query, ok := args["query"].(string)
	if !ok || query == "" {
		errMsg := "Please provide a query to search for"
		if mcpServer != nil {
			_ = mcpServer.SendLogMessageToClient(ctx, mcp.NewLoggingMessageNotification(
				mcp.LoggingLevelError,
				"api_handler",
				errMsg,
			))
		}
		return map[string]interface{}{
			"success": false,
			"error":   errMsg,
		}, nil
	}

	model, _ := args["model"].(string) //nolint:errcheck // Type assertion ok to ignore
	if model == "" {
		model = defaultModel
	}

	effort, _ := args["reasoning_effort"].(string) //nolint:errcheck // Type assertion ok to ignore
	effort = validateEffort(effort)

	verbosity, _ := args["verbosity"].(string) //nolint:errcheck // Type assertion ok to ignore
	verbosity = validateVerbosity(verbosity)

	previousResponseID, _ := args["previous_response_id"].(string) //nolint:errcheck // Type assertion ok to ignore

	// Use effort-based timeout
	timeout := getTimeoutForEffort(effort)

	// Make API call
	apiResp, err := CallAPI(ctx, apiKey, baseURL, query, model, effort, verbosity, previousResponseID, timeout)
	if err != nil {
		return nil, err
	}

	// Extract answer from response
	answer := ExtractAnswer(apiResp)
	if answer == "" {
		errMsg := "No answer found in response"
		if mcpServer != nil {
			_ = mcpServer.SendLogMessageToClient(ctx, mcp.NewLoggingMessageNotification(
				mcp.LoggingLevelWarning,
				"api_handler",
				errMsg,
			))
		}
		return map[string]interface{}{
			"success": false,
			"error":   errMsg,
		}, nil
	}

	// Log successful completion
	if mcpServer != nil {
		_ = mcpServer.SendLogMessageToClient(ctx, mcp.NewLoggingMessageNotification(
			mcp.LoggingLevelDebug,
			"api_handler",
			fmt.Sprintf("Search completed successfully, answer length: %d characters", len(answer)),
		))
	}

	// Return structured response
	return map[string]interface{}{
		"success":          true,
		"answer":           answer,
		"query":            query,
		"model":            apiResp.Model,
		"effort":           apiResp.Reasoning.Effort,
		"timeout_used":     timeout.String(),
		"id":               apiResp.ID,
		"requested_model":  model,
		"requested_effort": effort,
	}, nil
}
</file>

<file path="transport.go">
package main

import (
	"fmt"
	"log"

	"github.com/mark3labs/mcp-go/server"
)

// RunStdioTransport runs the MCP server using STDIO transport
func RunStdioTransport(mcpServer *server.MCPServer) error {
	log.Println("Starting STDIO transport...")
	return server.ServeStdio(mcpServer)
}

// RunHTTPTransport runs the MCP server using HTTP/SSE transport
func RunHTTPTransport(mcpServer *server.MCPServer, port string) error {
	httpServer := server.NewStreamableHTTPServer(mcpServer)

	addr := fmt.Sprintf(":%s", port)
	log.Printf("Starting HTTP/SSE server on %s", addr)
	log.Printf("MCP endpoint: http://localhost:%s/", port)

	return httpServer.Start(addr)
}
</file>

<file path="config.go">
package main

import (
	"os"
	"strconv"
	"time"
)

const (
	// Default values
	defaultModel     = "gpt-5-mini"
	defaultEffort    = "medium"
	defaultVerbosity = "medium"
	defaultBaseURL   = "https://api.openai.com/v1/responses"

	// Server metadata
	serverName    = "gpt-websearch-mcp"
	serverVersion = "1.0.0"

	// Timeouts based on reasoning effort
	timeoutMinimal = 90 * time.Second
	timeoutLow     = 3 * time.Minute
	timeoutMedium  = 5 * time.Minute
	timeoutHigh    = 10 * time.Minute
)

// API request/response structures
type reqReasoning struct {
	Effort string `json:"effort"`
}

type reqTool struct {
	Type string `json:"type"`
}

type reqText struct {
	Verbosity string `json:"verbosity"`
}

type requestBody struct {
	Model              string       `json:"model"`
	Input              string       `json:"input"`
	Reasoning          reqReasoning `json:"reasoning"`
	Text               reqText      `json:"text"`
	Tools              []reqTool    `json:"tools"`
	PreviousResponseID string       `json:"previous_response_id,omitempty"`
}

type respContent struct {
	Type string `json:"type"`
	Text string `json:"text"`
}

type respItem struct {
	Type    string        `json:"type"`
	Content []respContent `json:"content,omitempty"`
}

type apiResponse struct {
	ID        string       `json:"id"`
	Model     string       `json:"model"`
	Reasoning apiReasoning `json:"reasoning"`
	Output    []respItem   `json:"output"`
}

type apiReasoning struct {
	Effort string `json:"effort"`
}

// EnvConfig centralizes environment-derived configuration.
type EnvConfig struct {
	Question   string
	Model      string
	Effort     string
	ShowAll    bool
	HasShowAll bool
	Timeout    time.Duration
	HasTimeout bool
	APIKey     string
}

// MCPConfig holds configuration for the MCP server
type MCPConfig struct {
	APIKey    string
	BaseURL   string
	Transport string
	Port      string
	Verbose   bool
}

// loadEnvConfig reads environment variables
func loadEnvConfig() (EnvConfig, error) {
	cfg := EnvConfig{
		Question: os.Getenv("QUESTION"),
		Model:    os.Getenv("MODEL"),
		Effort:   os.Getenv("EFFORT"),
	}

	if v := os.Getenv("SHOW_ALL"); v != "" {
		if b, err := strconv.ParseBool(v); err == nil {
			cfg.ShowAll = b
			cfg.HasShowAll = true
		}
	}

	if v := os.Getenv("TIMEOUT"); v != "" {
		if d, err := time.ParseDuration(v); err == nil {
			cfg.Timeout = d
			cfg.HasTimeout = true
		}
	}

	cfg.APIKey = os.Getenv("OPENAI_API_KEY")
	if cfg.APIKey == "" {
		return EnvConfig{}, ErrNoAPIKey
	}

	return cfg, nil
}

// getTimeoutForEffort returns the appropriate timeout based on reasoning effort level
func getTimeoutForEffort(effort string) time.Duration {
	switch effort {
	case "high":
		return timeoutHigh
	case "medium":
		return timeoutMedium
	case "low", "":
		return timeoutLow
	case "minimal":
		return timeoutMinimal
	default:
		return timeoutLow
	}
}

// validateEffort ensures the effort level is valid
func validateEffort(effort string) string {
	switch effort {
	case "minimal", "low", "medium", "high":
		return effort
	case "":
		return defaultEffort
	default:
		return defaultEffort
	}
}

// validateVerbosity ensures the verbosity level is valid
func validateVerbosity(verbosity string) string {
	switch verbosity {
	case "low", "medium", "high":
		return verbosity
	case "":
		return defaultVerbosity
	default:
		return defaultVerbosity
	}
}

// parseMCPConfig creates MCPConfig from environment and command line flags
func parseMCPConfig(apiKey, baseURL, transport, port string, verbose bool) MCPConfig {
	// Use defaults if not provided
	if baseURL == "" {
		baseURL = defaultBaseURL
	}
	if transport == "" {
		transport = "stdio"
	}
	if port == "" {
		port = "8080"
	}

	return MCPConfig{
		APIKey:    apiKey,
		BaseURL:   baseURL,
		Transport: transport,
		Port:      port,
		Verbose:   verbose,
	}
}
</file>

<file path="main.go">
package main

import (
	"context"
	"encoding/json"
	"flag"
	"fmt"
	"os"
	"time"

	_ "github.com/joho/godotenv/autoload"
)

func main() {
	// Check if this is MCP server mode
	if len(os.Args) > 1 && os.Args[1] == "mcp" {
		runMCPMode()
		return
	}

	// Original CLI mode
	runCLI()
}

func runMCPMode() {
	// Create a new flag set for MCP subcommand
	mcpFlags := flag.NewFlagSet("mcp", flag.ExitOnError)

	var (
		transport = mcpFlags.String("t", "stdio", "Transport type (stdio or http)")
		port      = mcpFlags.String("port", "8080", "HTTP server port")
		baseURL   = mcpFlags.String("base", defaultBaseURL, "API base URL")
		verbose   = mcpFlags.Bool("verbose", false, "Enable verbose logging")
	)

	// Also support long form for transport
	transportLong := mcpFlags.String("transport", "", "Transport type (overrides -t)")

	// Parse MCP-specific flags (skip "answer mcp" args)
	if err := mcpFlags.Parse(os.Args[2:]); err != nil {
		fmt.Fprintf(os.Stderr, "Error parsing flags: %v\n", err)
		os.Exit(1)
	}

	// Use long form if provided
	if *transportLong != "" {
		*transport = *transportLong
	}

	// Load environment config
	envCfg, err := loadEnvConfig()
	if err != nil {
		fmt.Fprintf(os.Stderr, "Failed to load config: %v\n", err)
		os.Exit(1)
	}

	// Create server configuration using the config helper
	cfg := parseMCPConfig(envCfg.APIKey, *baseURL, *transport, *port, *verbose)

	// Create and run MCP server
	mcpServer := NewMCPServer(cfg)

	// Run with appropriate transport
	switch cfg.Transport {
	case "stdio":
		if err := RunStdioTransport(mcpServer); err != nil {
			fmt.Fprintf(os.Stderr, "STDIO transport error: %v\n", err)
			os.Exit(1)
		}
	case "http":
		if err := RunHTTPTransport(mcpServer, cfg.Port); err != nil {
			fmt.Fprintf(os.Stderr, "HTTP transport error: %v\n", err)
			os.Exit(1)
		}
	default:
		fmt.Fprintf(os.Stderr, "Unknown transport: %s (use 'stdio' or 'http')\n", cfg.Transport)
		os.Exit(1)
	}
}

func runCLI() {
	// Load environment-backed configuration
	envCfg, err := loadEnvConfig()
	if err != nil {
		fail(2, err.Error())
	}

	// Parse CLI flags
	var (
		baseURL = flag.String("base", defaultBaseURL, "API endpoint")
		model   = flag.String("model", func() string {
			if envCfg.Model != "" {
				return envCfg.Model
			}
			return defaultModel
		}(), "model (env MODEL)")
		effort = flag.String("effort", func() string {
			if envCfg.Effort != "" {
				return envCfg.Effort
			}
			return defaultEffort
		}(), "effort (env EFFORT)")
		verbosity   = flag.String("verbosity", defaultVerbosity, "response verbosity (low, medium, high)")
		questionVal string
		timeout     = flag.Duration("timeout", func() time.Duration {
			if envCfg.HasTimeout {
				return envCfg.Timeout
			}
			return getTimeoutForEffort(*effort)
		}(), "HTTP timeout (env TIMEOUT)")
		showAll = flag.Bool("show-all", func() bool {
			if envCfg.HasShowAll {
				return envCfg.ShowAll
			}
			return false
		}(), "print raw JSON response (env SHOW_ALL)")
	)
	flag.StringVar(&questionVal, "q", envCfg.Question, "question prompt (env QUESTION)")
	flag.StringVar(&questionVal, "question", envCfg.Question, "same as -q (env QUESTION)")
	flag.Parse()

	// Determine the final question value
	q := questionVal
	var questionFlagSet bool
	flag.Visit(func(f *flag.Flag) {
		if f.Name == "q" || f.Name == "question" {
			questionFlagSet = true
		}
	})
	if !questionFlagSet {
		if flag.NArg() > 0 {
			q = flag.Arg(0)
		}
	}
	if q == "" {
		fail(2, "please provide a question to ask (use -q flag or positional argument)")
	}

	// Validate effort and verbosity parameters
	*effort = validateEffort(*effort)
	*verbosity = validateVerbosity(*verbosity)

	// If timeout wasn't explicitly set, use effort-based timeout
	if !envCfg.HasTimeout {
		flag.Visit(func(f *flag.Flag) {
			if f.Name == "timeout" {
				return // User set it explicitly
			}
		})
		*timeout = getTimeoutForEffort(*effort)
	}

	// Make API call
	ctx := context.Background()
	apiResp, err := CallAPI(ctx, envCfg.APIKey, *baseURL, q, *model, *effort, *verbosity, "", *timeout)
	if err != nil {
		fail(2, err.Error())
	}

	if *showAll {
		// Print the full raw JSON when requested
		raw, _ := json.MarshalIndent(apiResp, "", "  ") //nolint:errcheck // Debug output, error ok to ignore
		fmt.Println(string(raw))
		return
	}

	// Extract and print the answer
	answer := ExtractAnswer(apiResp)
	if answer == "" {
		fail(3, "no answer found in response")
	}
	fmt.Println(answer)
}
</file>

<file path="mcp_server.go">
package main

import (
	"context"
	"encoding/json"
	"fmt"

	"github.com/mark3labs/mcp-go/mcp"
	"github.com/mark3labs/mcp-go/server"
)

// NewMCPServer creates and configures an MCP server with tools, resources, and prompts
func NewMCPServer(cfg MCPConfig) *server.MCPServer {
	// Create MCP server with capabilities
	mcpServer := server.NewMCPServer(
		serverName,
		serverVersion,
		server.WithLogging(),
		server.WithToolCapabilities(true),
		server.WithResourceCapabilities(true, false),
		server.WithPromptCapabilities(true),
	)

	// Add web search tool
	mcpServer.AddTool(
		mcp.NewTool("gpt_websearch",
			mcp.WithDescription("Search the web using OpenAI's GPT model with web search capabilities"),
			mcp.WithString("query",
				mcp.Required(),
				mcp.Description("The search query or question to ask"),
			),
			mcp.WithString("model",
				mcp.DefaultString(defaultModel),
				mcp.Description("The GPT model to use (default: gpt-5-mini)"),
			),
			mcp.WithString("reasoning_effort",
				mcp.DefaultString(defaultEffort),
				mcp.Description("Reasoning effort level: minimal (90s), low (3min), medium (5min), or high (10min timeout)"),
				mcp.Enum("minimal", "low", "medium", "high"),
			),
			mcp.WithString("verbosity",
				mcp.DefaultString(defaultVerbosity),
				mcp.Description("Response verbosity level: low (concise), medium (balanced), or high (detailed with explanations)"),
				mcp.Enum("low", "medium", "high"),
			),
			mcp.WithString("previous_response_id",
				mcp.Description("Optional: Previous response ID for conversation continuity - improves performance by avoiding re-reasoning"),
			),
		),
		webSearchHandler(cfg.APIKey, cfg.BaseURL),
	)

	// Add server info resource
	mcpServer.AddResource(
		mcp.NewResource(
			"server://info",
			"Server Information",
			mcp.WithResourceDescription("Information about the GPT Web Search MCP server"),
			mcp.WithMIMEType("text/plain"),
		),
		serverInfoHandler(cfg.BaseURL),
	)

	// Add intelligent web search prompt
	mcpServer.AddPrompt(
		mcp.NewPrompt("web_search",
			mcp.WithPromptDescription("Use the gpt_websearch tool to answer user questions based on web searching"),
			mcp.WithArgument("user_question",
				mcp.RequiredArgument(),
				mcp.ArgumentDescription("The question, task, problem, or instructions from the user that requires web search"),
			),
		),
		webSearchPromptHandler(),
	)

	return mcpServer
}

// webSearchHandler returns a handler for the web search tool
func webSearchHandler(apiKey, baseURL string) func(context.Context, mcp.CallToolRequest) (*mcp.CallToolResult, error) {
	return func(ctx context.Context, request mcp.CallToolRequest) (*mcp.CallToolResult, error) {
		// Get the server from context to send log messages
		mcpServer := server.ServerFromContext(ctx)

		// Extract parameters
		query, err := request.RequireString("query")
		if err != nil {
			if mcpServer != nil {
				_ = mcpServer.SendLogMessageToClient(ctx, mcp.NewLoggingMessageNotification(
					mcp.LoggingLevelError,
					"web_search",
					fmt.Sprintf("Failed to extract query parameter: %v", err),
				))
			}
			return mcp.NewToolResultError(err.Error()), nil
		}

		model := request.GetString("model", defaultModel)
		effort := request.GetString("reasoning_effort", defaultEffort)
		verbosity := request.GetString("verbosity", defaultVerbosity)
		previousResponseID := request.GetString("previous_response_id", "")

		// Log the search request
		if mcpServer != nil {
			_ = mcpServer.SendLogMessageToClient(ctx, mcp.NewLoggingMessageNotification(
				mcp.LoggingLevelInfo,
				"web_search",
				fmt.Sprintf("Executing web search: query='%s', model='%s', effort='%s', verbosity='%s'", query, model, effort, verbosity),
			))
		}

		// Call handler with properly extracted values
		args := map[string]interface{}{
			"query":                query,
			"model":                model,
			"reasoning_effort":     effort,
			"verbosity":            verbosity,
			"previous_response_id": previousResponseID,
		}

		result, err := HandleWebSearch(ctx, apiKey, baseURL, args)
		if err != nil {
			if mcpServer != nil {
				_ = mcpServer.SendLogMessageToClient(ctx, mcp.NewLoggingMessageNotification(
					mcp.LoggingLevelError,
					"web_search",
					fmt.Sprintf("Web search failed: %v", err),
				))
			}
			return mcp.NewToolResultError(err.Error()), nil
		}

		// Log success
		if mcpServer != nil {
			_ = mcpServer.SendLogMessageToClient(ctx, mcp.NewLoggingMessageNotification(
				mcp.LoggingLevelInfo,
				"web_search",
				"Web search completed successfully",
			))
		}

		// Convert result to JSON string for text content
		resultJSON, err := json.Marshal(result)
		if err != nil {
			if mcpServer != nil {
				_ = mcpServer.SendLogMessageToClient(ctx, mcp.NewLoggingMessageNotification(
					mcp.LoggingLevelError,
					"web_search",
					fmt.Sprintf("Failed to marshal result: %v", err),
				))
			}
			return mcp.NewToolResultError(fmt.Sprintf("failed to marshal result: %v", err)), nil
		}
		return mcp.NewToolResultText(string(resultJSON)), nil
	}
}

// serverInfoHandler returns a handler for the server info resource
func serverInfoHandler(baseURL string) func(context.Context, mcp.ReadResourceRequest) ([]mcp.ResourceContents, error) {
	return func(ctx context.Context, request mcp.ReadResourceRequest) ([]mcp.ResourceContents, error) {
		// Get the server from context to send log messages
		mcpServer := server.ServerFromContext(ctx)

		// Log the resource access
		if mcpServer != nil {
			_ = mcpServer.SendLogMessageToClient(ctx, mcp.NewLoggingMessageNotification(
				mcp.LoggingLevelDebug,
				"server_info",
				fmt.Sprintf("Server info resource accessed: URI=%s", request.Params.URI),
			))
		}

		info := fmt.Sprintf("GPT Web Search MCP Server\nVersion: %s\nEndpoint: %s\n", serverVersion, baseURL)
		return []mcp.ResourceContents{
			mcp.TextResourceContents{
				URI:      request.Params.URI,
				MIMEType: "text/plain",
				Text:     info,
			},
		}, nil
	}
}

// webSearchPromptHandler returns a handler for the intelligent web search prompt
func webSearchPromptHandler() func(context.Context, mcp.GetPromptRequest) (*mcp.GetPromptResult, error) {
	return func(ctx context.Context, request mcp.GetPromptRequest) (*mcp.GetPromptResult, error) {
		// Get the server from context to send log messages
		mcpServer := server.ServerFromContext(ctx)

		userQuestion := request.Params.Arguments["user_question"]
		if userQuestion == "" {
			if mcpServer != nil {
				_ = mcpServer.SendLogMessageToClient(ctx, mcp.NewLoggingMessageNotification(
					mcp.LoggingLevelError,
					"web_search_prompt",
					"user_question parameter is required",
				))
			}
			return nil, fmt.Errorf("user_question parameter is required")
		}

		// Log the prompt request
		if mcpServer != nil {
			_ = mcpServer.SendLogMessageToClient(ctx, mcp.NewLoggingMessageNotification(
				mcp.LoggingLevelDebug,
				"web_search_prompt",
				fmt.Sprintf("Generating prompt for question: %s", userQuestion),
			))
		}

		// Return properly structured messages with system and user roles
		messages := []mcp.PromptMessage{
			{
				Role: "user",
				Content: mcp.TextContent{
					Type: "text",
					Text: webSearchPrompt + "\n<user_question>\n" + userQuestion + "\n</user_question>\n",
				},
			},
		}

		return &mcp.GetPromptResult{
			Messages: messages,
		}, nil
	}
}
</file>

</files>
