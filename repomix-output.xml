This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.env.example
.gitignore
api.go
CLAUDE.md
config.go
errors.go
go.mod
integration_test.go
main.go
mcp_server.go
README.md
repo-map.txt
run_format.sh
run_lint.sh
run_test.sh
transport.go
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="CLAUDE.md">
# CLAUDE.md

LLM guidance for working with Answer - dual-mode Go application (CLI + MCP server) for OpenAI web search.

## Build Commands
- `go build -o bin/answer .`
- `./run_format.sh` - format code 
- `./run_lint.sh` - lint (golangci-lint)
- `./run_test.sh` - run tests
- `./test_timeouts.sh` - timeout behavior testing

## Environment
Required: `OPENAI_API_KEY`
Optional: `MODEL`, `EFFORT`, `SHOW_ALL`, `TIMEOUT`, `QUESTION`
Uses godotenv for `.env` loading

## Architecture
**Dual-mode design**: CLI or MCP server based on args
- CLI: `./bin/answer "query"`
- MCP: `./bin/answer mcp [options]`

**Key files**:
- `main.go` - entry point, mode routing
- `mcp_server.go` - MCP server implementation (commit b6c3478 enhanced prompts)
- `api.go` - OpenAI API integration  
- `config.go` - environment config, timeouts
- `transport.go` - stdio/HTTP transports
- `errors.go` - error handling

## Recent Changes (commit b6c3478)
**Enhanced MCP Prompt System**: Replaced basic `web_search` with `intelligent_web_search`
- **Name**: `intelligent_web_search` (was `web_search`)
- **Argument**: `user_question` (was `topic`)
- **System Message**: Comprehensive LLM instructions for cost-effective tool usage

**Model Selection Logic**:
- `gpt-5-nano`: Simple facts, definitions, summaries
- `gpt-5-mini`: Research, comparisons, specific topics
- `gpt-5`: Complex analysis, coding, reasoning

**Reasoning Effort**:
- `low`: 3min timeout, factual queries
- `medium`: 5min timeout, synthesis tasks
- `high`: 10min timeout, complex analysis

**Search Strategy**: Single/sequential/parallel approaches based on query complexity

## MCP Implementation Details

**Tool**: `gpt_websearch` - web search using GPT models with:
- Model selection: gpt-5-nano/mini/full based on complexity
- Reasoning effort: low/medium/high with timeout mapping
- Query formulation: context-aware, detailed searches
- Strategy: single/sequential/parallel based on task

**Prompt Template**: `intelligent_web_search` (mcp_server.go:139-213) provides:
- Systematic LLM instructions for cost-effective tool usage
- Model selection guidelines by task complexity
- Search strategy optimization (single/sequential/parallel)
- Query formulation best practices

**Transports**: 
- STDIO: Claude Desktop integration
- HTTP/SSE: Web applications (port 8080)

## API Integration
- Endpoint: `https://api.openai.com/v1/responses`
- Tool type: `web_search_preview`
- Models: gpt-5, gpt-5-mini, gpt-5-nano
- Effort-based timeouts: 3/5/10 minutes

## Error Handling
- CLI: `fail()` function with exit codes (errors.go)
- MCP: Structured JSON responses
- API errors: Custom `APIError` type wrapping

## Configuration Priority
1. CLI flags
2. Environment variables  
3. Defaults (gpt-5, low effort, 3min timeout)

## Testing
- `integration_test.go` - core API functions
- `test_timeouts.sh` - timeout behavior
- Environment-aware skipping for missing API keys

## Development Workflow
1. Set `OPENAI_API_KEY`
2. Make changes
3. `./run_format.sh && ./run_lint.sh && ./run_test.sh`
4. `go build -o bin/answer .`
5. Test CLI: `./bin/answer "query"`
6. Test MCP: `./bin/answer mcp -t stdio`

## Dependencies
- `github.com/mark3labs/mcp-go` v0.37.0 - MCP protocol
- `github.com/joho/godotenv` v1.5.1 - environment loading
</file>

<file path="integration_test.go">
package main

import (
	"context"
	"encoding/json"
	"testing"
	"time"
)

func TestGetTimeoutForEffort(t *testing.T) {
	tests := []struct {
		name     string
		effort   string
		expected time.Duration
	}{
		{"low effort", "low", timeoutLow},
		{"medium effort", "medium", timeoutMedium},
		{"high effort", "high", timeoutHigh},
		{"empty effort defaults to low", "", timeoutLow},
		{"invalid effort defaults to low", "invalid", timeoutLow},
		{"uppercase effort defaults to low", "HIGH", timeoutLow},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			got := getTimeoutForEffort(tt.effort)
			if got != tt.expected {
				t.Errorf("getTimeoutForEffort(%q) = %v, want %v", tt.effort, got, tt.expected)
			}
		})
	}
}

func TestValidateEffort(t *testing.T) {
	tests := []struct {
		name     string
		effort   string
		expected string
	}{
		{"valid low", "low", "low"},
		{"valid medium", "medium", "medium"},
		{"valid high", "high", "high"},
		{"empty returns default", "", defaultEffort},
		{"invalid returns default", "invalid", defaultEffort},
		{"uppercase returns default", "HIGH", defaultEffort},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			got := validateEffort(tt.effort)
			if got != tt.expected {
				t.Errorf("validateEffort(%q) = %q, want %q", tt.effort, got, tt.expected)
			}
		})
	}
}

func TestExtractAnswer(t *testing.T) {
	tests := []struct {
		name     string
		response *apiResponse
		expected string
	}{
		{
			name: "single output text",
			response: &apiResponse{
				Output: []respItem{
					{
						Type: "message",
						Content: []respContent{
							{Type: "output_text", Text: "This is the answer"},
						},
					},
				},
			},
			expected: "This is the answer",
		},
		{
			name: "multiple output texts joined",
			response: &apiResponse{
				Output: []respItem{
					{
						Type: "message",
						Content: []respContent{
							{Type: "output_text", Text: "First part"},
							{Type: "output_text", Text: "Second part"},
						},
					},
				},
			},
			expected: "First part Second part",
		},
		{
			name: "no message type returns empty",
			response: &apiResponse{
				Output: []respItem{
					{
						Type: "error",
						Content: []respContent{
							{Type: "output_text", Text: "Error message"},
						},
					},
				},
			},
			expected: "",
		},
		{
			name: "no output_text type returns empty",
			response: &apiResponse{
				Output: []respItem{
					{
						Type: "message",
						Content: []respContent{
							{Type: "error", Text: "Error message"},
						},
					},
				},
			},
			expected: "",
		},
		{
			name:     "empty response returns empty",
			response: &apiResponse{Output: []respItem{}},
			expected: "",
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			got := ExtractAnswer(tt.response)
			if got != tt.expected {
				t.Errorf("extractAnswer() = %q, want %q", got, tt.expected)
			}
		})
	}
}

func TestHandleWebSearch(t *testing.T) {
	// Skip if no API key is set
	if _, err := loadEnvConfig(); err != nil {
		t.Skip("Skipping web search test: OPENAI_API_KEY not set")
	}

	ctx := context.Background()
	apiKey := "test-key"
	baseURL := "https://api.openai.com/v1/responses"

	tests := []struct {
		name          string
		args          map[string]interface{}
		expectSuccess bool
		checkFields   []string
	}{
		{
			name:          "empty query returns error",
			args:          map[string]interface{}{},
			expectSuccess: false,
			checkFields:   []string{"error"},
		},
		{
			name: "valid query with defaults",
			args: map[string]interface{}{
				"query": "test query",
			},
			expectSuccess: true,
			checkFields:   []string{"answer", "query", "model", "effort", "timeout_used"},
		},
		{
			name: "valid query with custom effort",
			args: map[string]interface{}{
				"query":            "test query",
				"reasoning_effort": "high",
			},
			expectSuccess: true,
			checkFields:   []string{"answer", "effort", "timeout_used"},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			// Note: This will fail with actual API calls unless valid key is provided
			// For unit testing, you'd typically mock the API call
			result, err := HandleWebSearch(ctx, apiKey, baseURL, tt.args)

			// Check if we got any result
			if err != nil && tt.expectSuccess {
				// Skip if it's an API error (likely invalid key for testing)
				if _, ok := err.(*APIError); ok {
					t.Skip("Skipping due to API error (likely test environment)")
				}
				t.Errorf("handleWebSearch() error = %v, expectSuccess = %v", err, tt.expectSuccess)
				return
			}

			if result == nil {
				t.Fatal("handleWebSearch() returned nil result")
			}

			resultMap, ok := result.(map[string]interface{})
			if !ok {
				t.Fatal("handleWebSearch() result is not a map")
			}

			// Check success field
			success, _ := resultMap["success"].(bool)
			if success != tt.expectSuccess {
				t.Errorf("handleWebSearch() success = %v, want %v", success, tt.expectSuccess)
			}

			// Check expected fields exist
			for _, field := range tt.checkFields {
				if _, exists := resultMap[field]; !exists {
					t.Errorf("handleWebSearch() missing expected field %q", field)
				}
			}

			// Specific checks for successful responses
			if tt.expectSuccess && success {
				// Check answer is a string, not an array
				if answer, exists := resultMap["answer"]; exists {
					if _, ok := answer.(string); !ok {
						t.Errorf("handleWebSearch() answer is not a string, got type %T", answer)
					}
				}

				// Check timeout_used matches effort
				if effort, exists := resultMap["effort"]; exists {
					expectedTimeout := getTimeoutForEffort(effort.(string))
					if timeoutUsed, exists := resultMap["timeout_used"]; exists {
						if timeoutUsed != expectedTimeout.String() {
							t.Errorf("handleWebSearch() timeout_used = %v, want %v for effort %v",
								timeoutUsed, expectedTimeout.String(), effort)
						}
					}
				}
			}

			// For error responses, check error message
			if !tt.expectSuccess && !success {
				if errMsg, exists := resultMap["error"]; exists {
					if _, ok := errMsg.(string); !ok {
						t.Errorf("handleWebSearch() error is not a string, got type %T", errMsg)
					}
				} else {
					t.Error("handleWebSearch() error response missing 'error' field")
				}
			}
		})
	}
}

func TestRequestBodyJSON(t *testing.T) {
	body := requestBody{
		Model: "gpt-5",
		Input: "test query",
		Reasoning: reqReasoning{
			Effort: "medium",
		},
		Tools: []reqTool{
			{Type: "web_search_preview"},
		},
	}

	data, err := json.Marshal(body)
	if err != nil {
		t.Fatalf("Failed to marshal requestBody: %v", err)
	}

	var decoded requestBody
	if err := json.Unmarshal(data, &decoded); err != nil {
		t.Fatalf("Failed to unmarshal requestBody: %v", err)
	}

	if decoded.Model != body.Model {
		t.Errorf("Model mismatch: got %q, want %q", decoded.Model, body.Model)
	}
	if decoded.Input != body.Input {
		t.Errorf("Input mismatch: got %q, want %q", decoded.Input, body.Input)
	}
	if decoded.Reasoning.Effort != body.Reasoning.Effort {
		t.Errorf("Effort mismatch: got %q, want %q", decoded.Reasoning.Effort, body.Reasoning.Effort)
	}
	if len(decoded.Tools) != 1 || decoded.Tools[0].Type != "web_search_preview" {
		t.Errorf("Tools mismatch: got %v", decoded.Tools)
	}
}

func TestParseTimeoutWithEffort(t *testing.T) {
	tests := []struct {
		name       string
		timeoutStr string
		effort     string
		expected   time.Duration
	}{
		{"empty timeout uses effort low", "", "low", timeoutLow},
		{"empty timeout uses effort medium", "", "medium", timeoutMedium},
		{"empty timeout uses effort high", "", "high", timeoutHigh},
		{"valid timeout overrides effort", "30s", "high", 30 * time.Second},
		{"invalid timeout falls back to effort", "invalid", "medium", timeoutMedium},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			got := parseTimeoutWithEffort(tt.timeoutStr, tt.effort)
			if got != tt.expected {
				t.Errorf("parseTimeoutWithEffort(%q, %q) = %v, want %v",
					tt.timeoutStr, tt.effort, got, tt.expected)
			}
		})
	}
}
</file>

<file path="README.md">
# Answer - GPT Web Search CLI & MCP Server

A Go application that provides intelligent web search capabilities using OpenAI's GPT models. Works as both a CLI tool and MCP (Model Context Protocol) server with cost-effective model selection.

## Features

- üîç **Intelligent Web Search**: Uses OpenAI's GPT models (gpt-5, gpt-5-mini, gpt-5-nano) with web search capabilities
- üéØ **Cost-Effective**: Automatic model selection based on query complexity for optimal cost/performance
- üöÄ **Dual Mode**: CLI tool and MCP server with stdio/HTTP transports
- ‚öôÔ∏è **Smart Configuration**: Effort-based timeouts (3/5/10 minutes) and environment-driven setup
- üß† **Enhanced MCP Prompts**: Intelligent prompt templates guide optimal tool usage
- üîê **Secure**: Environment-based API key management

## Installation

### Prerequisites

- Go 1.24.0 or later
- OpenAI API key with web search preview access

### Build from Source

```bash
# Clone the repository
git clone <repository-url>
cd Answer

# Install dependencies
go mod download

# Build the binary
go build -o bin/answer .

# Or install globally
go install .
```

## Configuration

### Environment Variables

Create a `.env` file in the project root:

```env
OPENAI_API_KEY=your-api-key-here
MODEL=gpt-5              # Optional: gpt-5 (default), gpt-5-mini, gpt-5-nano
EFFORT=low               # Optional: reasoning effort (low/medium/high)
SHOW_ALL=false           # Optional: show raw JSON
QUESTION=                # Optional: default question
```

**Model Selection Guidelines**:
- `gpt-5-nano`: Simple facts, definitions, quick lookups
- `gpt-5-mini`: Research tasks, comparisons, specific topics  
- `gpt-5`: Complex analysis, coding questions, reasoning tasks

**Effort-Based Timeouts**: `low` = 3 minutes, `medium` = 5 minutes, `high` = 10 minutes.

## Usage

### CLI Mode

Use Answer as a command-line tool for direct web searches:

```bash
# Simple query with positional argument
./bin/answer "Who won the 2024 Super Bowl?"

# Using the -q flag
./bin/answer -q "Latest AI developments"

# Error if no query provided
./bin/answer  # Error: please provide a question to ask

# With custom model and effort
./bin/answer -q "Explain quantum computing" -model gpt-5-mini -effort high

# Show raw JSON response
./bin/answer -q "Test query" -show-all

# Custom timeout
./bin/answer -q "Complex analysis" -timeout 120s
```

### MCP Server Mode

Run Answer as an MCP server for integration with AI assistants:

#### STDIO Transport (for Claude Desktop)

```bash
# Start MCP server in stdio mode (default)
./bin/answer mcp

# Or explicitly specify stdio
./bin/answer mcp -t stdio
./bin/answer mcp --transport stdio
```

**Claude Desktop Configuration:**

Add to `~/Library/Application Support/Claude/claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "gpt-websearch": {
      "command": "/path/to/Answer/bin/answer",
      "args": ["mcp", "-t", "stdio"],
      "env": {
        "OPENAI_API_KEY": "your-api-key"
      }
    }
  }
}
```

#### HTTP/SSE Transport (for Web Integration)

```bash
# Start HTTP server on default port 8080
./bin/answer mcp -t http

# Custom port
./bin/answer mcp -t http -port 3000

# With verbose logging
./bin/answer mcp -t http -verbose
```

**Endpoints:**
- `GET /` - API documentation
- `GET /health` - Health check
- `GET /sse` - Server-Sent Events for MCP protocol
- `GET /message` - Message handling endpoint

## MCP Server Features

### Tool: `gpt_websearch`
Performs intelligent web searches with cost-effective model selection:

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `query` | string | Yes | - | The search query or question |
| `model` | string | No | `gpt-5` | GPT model: gpt-5, gpt-5-mini, or gpt-5-nano |
| `reasoning_effort` | string | No | `low` | Effort level:<br>`low` = 3 minutes<br>`medium` = 5 minutes<br>`high` = 10 minutes |

### Prompt: `web_search`
Enhanced prompt template that guides Claude Desktop to:
- Analyze user questions in conversation context
- Select cost-effective models based on complexity
- Choose appropriate reasoning effort levels
- Use single, sequential, or parallel search strategies

### Example Response

```json
{
  "success": true,
  "answer": "The complete answer to your query...",
  "query": "original query",
  "model": "gpt-5",
  "effort": "low",
  "timeout_used": "3m0s"
}
```

## Command-Line Reference

### CLI Mode
```
answer [options] [question]

Options:
  -q, -question    Question to ask (required, can also use positional argument)
  -model          Model: gpt-5 (default), gpt-5-mini, gpt-5-nano
  -effort         Reasoning effort: low (3min), medium (5min), high (10min timeout)
  -timeout        Request timeout (overrides effort-based defaults)
  -show-all       Show raw JSON response
  -base           API endpoint URL
```

### MCP Server Mode
```
answer mcp [options]

Options:
  -t, --transport  Transport type: stdio or http (default: stdio)
  -port           HTTP server port (default: 8080)
  -base           API endpoint URL
  -verbose        Enable verbose logging
```

## Examples

### CLI Examples

```bash
# Quick question
./bin/answer "What's the weather in San Francisco?"

# Research query with high effort
./bin/answer -q "Latest breakthroughs in quantum computing 2024" -effort high

# Using gpt-5-mini for research tasks
./bin/answer -q "Explain the theory of relativity" -model gpt-5-mini

# Debug mode with raw output
./bin/answer -q "Test query" -show-all
```

### MCP Integration Examples

**JavaScript SSE Client:**

```javascript
const eventSource = new EventSource('http://localhost:8080/sse');

eventSource.onmessage = (event) => {
  console.log('Received:', JSON.parse(event.data));
};

// Send request via fetch to message endpoint
fetch('http://localhost:8080/message', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    jsonrpc: '2.0',
    method: 'tools/call',
    params: {
      name: 'gpt_websearch',
      arguments: {
        query: 'Latest AI news',
        model: 'gpt-5-mini',
        reasoning_effort: 'medium'  // 5-minute timeout
      }
    },
    id: 1
  })
});
```

## Docker Support

```dockerfile
FROM golang:1.25-alpine AS builder
WORKDIR /app
COPY . .
RUN go mod download
RUN go build -o answer .

FROM alpine:latest
RUN apk --no-cache add ca-certificates
COPY --from=builder /app/answer /usr/local/bin/answer
ENV OPENAI_API_KEY=""
EXPOSE 8080
CMD ["answer", "mcp", "-t", "http"]
```

Build and run:
```bash
docker build -t answer .
docker run -p 8080:8080 -e OPENAI_API_KEY=your-key answer
```

## Development

### Project Structure
```
Answer/
‚îú‚îÄ‚îÄ main.go              # Main entry point with CLI and MCP modes
‚îú‚îÄ‚îÄ config.go           # Configuration structures and helpers
‚îú‚îÄ‚îÄ errors.go           # Error definitions
‚îú‚îÄ‚îÄ go.mod              # Go module definition
‚îú‚îÄ‚îÄ go.sum              # Dependency checksums
‚îú‚îÄ‚îÄ .env                # Environment variables (not committed)
‚îú‚îÄ‚îÄ bin/
‚îÇ   ‚îî‚îÄ‚îÄ answer         # Compiled binary
‚îú‚îÄ‚îÄ AGENTS.md          # Development guidelines
‚îú‚îÄ‚îÄ MCP_SERVER.md      # Detailed MCP documentation
‚îî‚îÄ‚îÄ README.md          # This file
```

### Building
```bash
go build -o bin/answer .
```

### Testing
```bash
go test ./...
```

### Formatting
```bash
go fmt ./...
```

## Troubleshooting

### Common Issues

1. **"OPENAI_API_KEY is not set"**
   - Set the environment variable or create a `.env` file

2. **"API error: status=401"**
   - Verify your API key is valid and has web search preview access

3. **"no output_text found in response"**
   - The model might not support web search
   - Try using a different model

4. **MCP client connection issues**
   - For stdio: Check that the binary path is correct
   - For HTTP: Verify the port is not in use

### Debug Mode

```bash
# CLI debug
./bin/answer -q "test" -show-all

# MCP debug
./bin/answer mcp -t stdio -verbose
./bin/answer mcp -t http -verbose
```

## License

See LICENSE file for details.

## Contributing

Please see [AGENTS.md](AGENTS.md) for development guidelines and contribution rules.

## Links

- [MCP Protocol Specification](https://modelcontextprotocol.io/)
- [OpenAI API Documentation](https://platform.openai.com/docs)
- [Detailed MCP Server Documentation](MCP_SERVER.md)
</file>

<file path="repo-map.txt">
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Newer aider version v0.86.1 is available.

/opt/homebrew/Cellar/aider/0.86.0/libexec/bin/python -m pip install --upgrade --upgrade-strategy only-if-needed aider-chat
Run pip install? (Y)es/(N)o [Yes]:  






                                                                                                                                                                                                  Run pip install? (Y)es/(N)o [Yes]:
</file>

<file path=".env.example">
# This file contains environment variables for the application.
# Copy this file to .env and fill in the values.

# The question to ask the model.
QUESTION="What is the meaning of life?"

# The model to use (e.g., gpt-5).
MODEL="gpt-5"

# Show the full JSON response from the API.
SHOW_ALL=false

# The effort level for the model.
EFFORT="medium"

# The timeout for the API request (e.g., 30s, 1m).
TIMEOUT="300s"

# Your OpenAI API key.
OPENAI_API_KEY=""
</file>

<file path="api.go">
package main

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"time"
)

// CallAPI makes the actual API call - reusable for both CLI and MCP
func CallAPI(ctx context.Context, apiKey, baseURL, query, model, effort string, timeout time.Duration) (*apiResponse, error) {
	body := requestBody{
		Model: model,
		Input: query,
		Reasoning: reqReasoning{
			Effort: effort,
		},
		Tools: []reqTool{
			{Type: "web_search_preview"},
		},
	}

	buf, err := json.Marshal(body)
	if err != nil {
		return nil, fmt.Errorf("marshal request: %w", err)
	}

	req, err := http.NewRequestWithContext(ctx, http.MethodPost, baseURL, bytes.NewReader(buf))
	if err != nil {
		return nil, fmt.Errorf("build request: %w", err)
	}
	req.Header.Set("Content-Type", "application/json")
	req.Header.Set("Authorization", "Bearer "+apiKey)

	client := &http.Client{Timeout: timeout}
	resp, err := client.Do(req)
	if err != nil {
		return nil, fmt.Errorf("http request: %w", err)
	}
	defer resp.Body.Close()

	bodyBytes, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, fmt.Errorf("read response: %w", err)
	}

	if resp.StatusCode < 200 || resp.StatusCode >= 300 {
		return nil, &APIError{StatusCode: resp.StatusCode, Body: string(bodyBytes)}
	}

	var ar apiResponse
	if err := json.Unmarshal(bodyBytes, &ar); err != nil {
		return nil, fmt.Errorf("parse json: %w", err)
	}

	return &ar, nil
}

// ExtractAnswer extracts the answer text from the API response
func ExtractAnswer(apiResp *apiResponse) string {
	var answers []string
	for _, item := range apiResp.Output {
		if item.Type != "message" {
			continue
		}
		for _, content := range item.Content {
			if content.Type == "output_text" && content.Text != "" {
				answers = append(answers, content.Text)
			}
		}
	}

	// Join all text content into a single answer
	if len(answers) > 0 {
		// If multiple text segments, join them with space
		answer := ""
		for i, text := range answers {
			if i > 0 {
				answer += " "
			}
			answer += text
		}
		return answer
	}
	return ""
}

// HandleWebSearch handles web search requests for the MCP server
func HandleWebSearch(ctx context.Context, apiKey, baseURL string, args map[string]interface{}) (interface{}, error) {
	// Extract parameters
	query, ok := args["query"].(string)
	if !ok || query == "" {
		return map[string]interface{}{
			"success": false,
			"error":   "Please provide a query to search for",
		}, nil
	}

	model, _ := args["model"].(string) //nolint:errcheck // Type assertion ok to ignore
	if model == "" {
		model = defaultModel
	}

	effort, _ := args["reasoning_effort"].(string) //nolint:errcheck // Type assertion ok to ignore
	effort = validateEffort(effort)

	// Use effort-based timeout
	timeout := getTimeoutForEffort(effort)

	// Make API call
	apiResp, err := CallAPI(ctx, apiKey, baseURL, query, model, effort, timeout)
	if err != nil {
		return nil, err
	}

	// Extract the answer from response
	answer := ExtractAnswer(apiResp)
	if answer == "" {
		return map[string]interface{}{
			"success": false,
			"error":   "No answer found in response",
		}, nil
	}

	// Return structured response
	return map[string]interface{}{
		"success":      true,
		"answer":       answer,
		"query":        query,
		"model":        model,
		"effort":       effort,
		"timeout_used": timeout.String(),
	}, nil
}
</file>

<file path="config.go">
package main

import (
	"os"
	"strconv"
	"time"
)

const (
	// Default values
	defaultModel   = "gpt-5"
	defaultEffort  = "low"
	defaultBaseURL = "https://api.openai.com/v1/responses"

	// Server metadata
	serverName    = "gpt-websearch-mcp"
	serverVersion = "1.0.0"

	// Timeouts based on reasoning effort
	timeoutLow    = 3 * time.Minute
	timeoutMedium = 5 * time.Minute
	timeoutHigh   = 10 * time.Minute
)

// API request/response structures
type reqReasoning struct {
	Effort string `json:"effort"`
}

type reqTool struct {
	Type string `json:"type"`
}

type requestBody struct {
	Model     string       `json:"model"`
	Input     string       `json:"input"`
	Reasoning reqReasoning `json:"reasoning"`
	Tools     []reqTool    `json:"tools"`
}

type respContent struct {
	Type string `json:"type"`
	Text string `json:"text"`
}

type respItem struct {
	Type    string        `json:"type"`
	Content []respContent `json:"content,omitempty"`
}

type apiResponse struct {
	Output []respItem `json:"output"`
}

// EnvConfig centralizes environment-derived configuration.
type EnvConfig struct {
	Question   string
	Model      string
	Effort     string
	ShowAll    bool
	HasShowAll bool
	Timeout    time.Duration
	HasTimeout bool
	APIKey     string
}

// loadEnvConfig reads environment variables
func loadEnvConfig() (EnvConfig, error) {
	cfg := EnvConfig{
		Question: os.Getenv("QUESTION"),
		Model:    os.Getenv("MODEL"),
		Effort:   os.Getenv("EFFORT"),
	}

	if v := os.Getenv("SHOW_ALL"); v != "" {
		if b, err := strconv.ParseBool(v); err == nil {
			cfg.ShowAll = b
			cfg.HasShowAll = true
		}
	}

	if v := os.Getenv("TIMEOUT"); v != "" {
		if d, err := time.ParseDuration(v); err == nil {
			cfg.Timeout = d
			cfg.HasTimeout = true
		}
	}

	cfg.APIKey = os.Getenv("OPENAI_API_KEY")
	if cfg.APIKey == "" {
		return EnvConfig{}, ErrNoAPIKey
	}

	return cfg, nil
}

// getTimeoutForEffort returns the appropriate timeout based on reasoning effort level
func getTimeoutForEffort(effort string) time.Duration {
	switch effort {
	case "high":
		return timeoutHigh
	case "medium":
		return timeoutMedium
	case "low", "":
		return timeoutLow
	default:
		return timeoutLow
	}
}

// parseTimeoutWithEffort parses a timeout string or returns effort-based default
func parseTimeoutWithEffort(timeoutStr, effort string) time.Duration {
	if timeoutStr != "" {
		if d, err := time.ParseDuration(timeoutStr); err == nil {
			return d
		}
	}
	return getTimeoutForEffort(effort)
}

// validateEffort ensures the effort level is valid
func validateEffort(effort string) string {
	switch effort {
	case "low", "medium", "high":
		return effort
	case "":
		return defaultEffort
	default:
		return defaultEffort
	}
}
</file>

<file path="run_format.sh">
#!/bin/sh

export PATH="/usr/local/go/bin:$PATH"

# Run gofmt to format all Go files recursively
/usr/local/go/bin/gofmt -w .
</file>

<file path="run_lint.sh">
#!/bin/sh

export PATH="/usr/local/go/bin:$PATH"
export HOME="/Users/rrj"
export GOLANGCI_LINT_CACHE="$HOME/Library/Caches/golangci-lint"
export GOCACHE="$HOME/.cache/go-build"

# Run linter
/Users/rrj/Projekty/Go/bin/golangci-lint run --fix ./...
</file>

<file path="run_test.sh">
#!/bin/sh

export PATH="/usr/local/go/bin:$PATH"

# Run tests
go test -v ./...
</file>

<file path="transport.go">
package main

import (
	"context"
	"fmt"
	"log"
	"net/http"
	"os"
	"time"

	"github.com/mark3labs/mcp-go/server"
)

// RunStdioTransport runs the MCP server using STDIO transport
func RunStdioTransport(ctx context.Context, mcpServer *server.MCPServer, sigChan chan os.Signal) error {
	// Create stdio server
	stdioServer := server.NewStdioServer(mcpServer)

	// Create error channel
	errChan := make(chan error, 1)

	// Run server in goroutine
	go func() {
		if err := stdioServer.Listen(ctx, os.Stdin, os.Stdout); err != nil {
			errChan <- fmt.Errorf("serve error: %w", err)
		}
	}()

	// Wait for shutdown signal or error
	select {
	case <-sigChan:
		log.Println("Shutting down stdio server...")
		return nil
	case err := <-errChan:
		return err
	case <-ctx.Done():
		return ctx.Err()
	}
}

// RunHTTPTransport runs the MCP server using HTTP/SSE transport
func RunHTTPTransport(ctx context.Context, mcpServer *server.MCPServer, port string, sigChan chan os.Signal) error {
	// Create SSE server for HTTP
	sseServer := server.NewSSEServer(mcpServer)

	// Setup HTTP routes
	mux := http.NewServeMux()

	// Health check endpoint
	mux.HandleFunc("/health", func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "application/json")
		w.WriteHeader(http.StatusOK)
		//nolint:errcheck // HTTP response write error handled by HTTP layer
		fmt.Fprintf(w, `{"status":"healthy","server":"%s","version":"%s"}`, serverName, serverVersion)
	})

	// SSE endpoint for MCP communication
	mux.HandleFunc("/sse", sseServer.SSEHandler().ServeHTTP)

	// Message endpoint for MCP communication
	mux.HandleFunc("/message", sseServer.MessageHandler().ServeHTTP)

	// API documentation endpoint
	mux.HandleFunc("/", func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/html")
		//nolint:errcheck // HTTP response write error handled by HTTP layer
		fmt.Fprintf(w, `<!DOCTYPE html>
<html>
<head>
    <title>GPT Web Search MCP Server</title>
    <style>
        body { font-family: -apple-system, system-ui, sans-serif; max-width: 800px; margin: 50px auto; padding: 20px; }
        h1 { color: #333; }
        .endpoint { background: #f5f5f5; padding: 10px; margin: 10px 0; border-radius: 5px; }
        code { background: #eee; padding: 2px 5px; border-radius: 3px; }
        .tool { background: #e8f4f8; padding: 15px; margin: 20px 0; border-radius: 5px; }
        .note { background: #fff3cd; padding: 10px; margin: 10px 0; border-radius: 5px; }
    </style>
</head>
<body>
    <h1>üîç GPT Web Search MCP Server</h1>
    <p>Version: %s</p>
    <h2>Available Endpoints</h2>
    <div class="endpoint">
        <strong>GET /health</strong> - Health check endpoint
    </div>
    <div class="endpoint">
        <strong>GET /sse</strong> - Server-Sent Events endpoint for MCP protocol
    </div>
    <h2>Available Tools</h2>
    <div class="tool">
        <h3>gpt_websearch</h3>
        <p>Search the web using OpenAI's GPT model with web search capabilities</p>
        <h4>Parameters:</h4>
        <ul>
            <li><code>query</code> (required) - The search query or question</li>
            <li><code>model</code> (optional) - GPT model to use (default: gpt-5)</li>
            <li><code>reasoning_effort</code> (optional) - Effort level with automatic timeout:
                <ul>
                    <li><code>low</code> - 3 minute timeout</li>
                    <li><code>medium</code> - 5 minute timeout</li>
                    <li><code>high</code> - 10 minute timeout</li>
                </ul>
            </li>
        </ul>
    </div>
    <div class="note">
        <strong>Note:</strong> API key must be set via <code>OPENAI_API_KEY</code> environment variable.
    </div>
</body>
</html>`, serverVersion)
	})

	// Create HTTP server
	httpServer := &http.Server{
		Addr:         ":" + port,
		Handler:      mux,
		ReadTimeout:  15 * time.Second,
		WriteTimeout: 15 * time.Second,
		IdleTimeout:  60 * time.Second,
	}

	// Start server in goroutine
	go func() {
		log.Printf("Starting HTTP server on port %s", port)
		log.Printf("Health check: http://localhost:%s/health", port)
		log.Printf("SSE endpoint: http://localhost:%s/sse", port)
		log.Printf("Documentation: http://localhost:%s/", port)

		if err := httpServer.ListenAndServe(); err != nil && err != http.ErrServerClosed {
			log.Fatalf("HTTP server error: %v", err)
		}
	}()

	// Wait for shutdown signal
	<-sigChan
	log.Println("Shutting down HTTP server...")

	// Graceful shutdown with timeout
	shutdownCtx, shutdownCancel := context.WithTimeout(ctx, 10*time.Second)
	defer shutdownCancel()

	if err := httpServer.Shutdown(shutdownCtx); err != nil {
		return fmt.Errorf("server shutdown error: %w", err)
	}

	return nil
}
</file>

<file path=".gitignore">
.DS_Store
bin/**
.env
go.sum
*.log

.aider*
.claude/**
.gemini/**
.vscode/**
.repomix/**

repomix.config.json

test_*
</file>

<file path="errors.go">
package main

import (
	"errors"
	"fmt"
	"os"
)

var (
	// Configuration errors
	ErrNoAPIKey = errors.New("OPENAI_API_KEY environment variable is required")

	// API errors
	ErrNoOutputText = errors.New("no output_text found in response")
	ErrAPIRequest   = errors.New("API request failed")

	// MCP errors
	ErrQueryRequired      = errors.New("please provide a query to search for")
	ErrInvalidEffort      = errors.New("invalid reasoning effort level")
	ErrSessionNotFound    = errors.New("session not found")
	ErrNotificationFailed = errors.New("failed to send notification")
)

// APIError represents an error from the OpenAI API
type APIError struct {
	StatusCode int
	Body       string
}

func (e *APIError) Error() string {
	return fmt.Sprintf("API error: status=%d body=%s", e.StatusCode, e.Body)
}

// fail prints to stderr and exits non-zero.
func fail(code int, msg string) {
	fmt.Fprintf(os.Stderr, "%s\n", msg)
	os.Exit(code)
}
</file>

<file path="mcp_server.go">
package main

import (
	"context"
	"encoding/json"
	"flag"
	"fmt"
	"log"
	"os"
	"os/signal"
	"syscall"

	"github.com/mark3labs/mcp-go/mcp"
	"github.com/mark3labs/mcp-go/server"
)

// RunMCPServer initializes and runs the MCP server
func RunMCPServer() {
	// Create a new flag set for MCP subcommand
	mcpFlags := flag.NewFlagSet("mcp", flag.ExitOnError)

	var (
		transport     = mcpFlags.String("t", "stdio", "Transport type")
		transportLong = mcpFlags.String("transport", "", "Transport type (overrides -t)")
		port          = mcpFlags.String("port", "8080", "HTTP server port")
		baseURL       = mcpFlags.String("base", defaultBaseURL, "API base URL")
		verbose       = mcpFlags.Bool("verbose", false, "Enable verbose logging")
	)

	// Parse MCP-specific flags (skip "answer mcp" args)
	mcpFlags.Parse(os.Args[2:]) //nolint:errcheck // Flag parsing error handling done by FlagSet

	// Use long form if provided
	if *transportLong != "" {
		*transport = *transportLong
	}

	// Configure logging
	if !*verbose {
		log.SetOutput(os.Stderr)
	}

	// Load environment config
	envCfg, err := loadEnvConfig()
	if err != nil {
		log.Fatalf("Failed to load config: %v", err)
	}

	// Create MCP server
	mcpServer := CreateMCPServer(envCfg.APIKey, *baseURL)

	// Setup context with cancellation
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	// Handle shutdown signals
	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, os.Interrupt, syscall.SIGTERM)

	switch *transport {
	case "stdio":
		if err := RunStdioTransport(ctx, mcpServer, sigChan); err != nil {
			log.Fatalf("STDIO transport error: %v", err)
		}
	case "http":
		if err := RunHTTPTransport(ctx, mcpServer, *port, sigChan); err != nil {
			log.Fatalf("HTTP transport error: %v", err)
		}
	default:
		log.Fatalf("Unknown transport: %s (use 'stdio' or 'http')", *transport)
	}
}

// CreateMCPServer creates and configures the MCP server with tools, resources, and prompts
func CreateMCPServer(apiKey, baseURL string) *server.MCPServer {
	// Create MCP server
	mcpServer := server.NewMCPServer(
		serverName,
		serverVersion,
	)

	// Create the tool definition
	tool := mcp.Tool{
		Name:        "gpt_websearch",
		Description: "Search the web using OpenAI's GPT model with web search capabilities",
		InputSchema: mcp.ToolInputSchema{
			Type: "object",
			Properties: map[string]interface{}{
				"query": map[string]interface{}{
					"type":        "string",
					"description": "The search query or question to ask",
				},
				"model": map[string]interface{}{
					"type":        "string",
					"description": "The GPT model to use (default: gpt-5)",
					"default":     defaultModel,
				},
				"reasoning_effort": map[string]interface{}{
					"type":        "string",
					"description": "Reasoning effort level: low (3min), medium (5min), or high (10min timeout)",
					"enum":        []string{"low", "medium", "high"},
					"default":     defaultEffort,
				},
			},
			Required: []string{"query"},
		},
	}

	// Register the tool with its handler
	mcpServer.AddTool(tool, func(ctx context.Context, request mcp.CallToolRequest) (*mcp.CallToolResult, error) {
		args := request.GetArguments()
		result, err := HandleWebSearch(ctx, apiKey, baseURL, args)
		if err != nil {
			return mcp.NewToolResultError(err.Error()), nil
		}
		// Convert result to JSON string for text content
		resultJSON, _ := json.Marshal(result) //nolint:errcheck // JSON marshal for simple types, error ok to ignore
		return mcp.NewToolResultText(string(resultJSON)), nil
	})

	// Add server info resource
	resource := mcp.Resource{
		URI:         "server-info",
		Name:        "server-info",
		Description: "Information about the MCP server",
		MIMEType:    "text/plain",
	}
	mcpServer.AddResource(resource, func(ctx context.Context, request mcp.ReadResourceRequest) ([]mcp.ResourceContents, error) {
		info := fmt.Sprintf("GPT Web Search MCP Server\nVersion: %s\nEndpoint: %s\n", serverVersion, baseURL)
		return []mcp.ResourceContents{
			&mcp.TextResourceContents{
				URI:      "server-info",
				MIMEType: "text/plain",
				Text:     info,
			},
		}, nil
	})

	// Add intelligent web search prompt
	prompt := mcp.Prompt{
		Name:        "web_search",
		Description: "Use the gpt_websearch tool to answer user questions based on web searching",
		Arguments: []mcp.PromptArgument{
			{
				Name:        "user_question",
				Description: "The question, task, problem, or instructions from the user that requires web search",
				Required:    true,
			},
		},
	}
	mcpServer.AddPrompt(prompt, func(ctx context.Context, request mcp.GetPromptRequest) (*mcp.GetPromptResult, error) {
		userQuestion, ok := request.Params.Arguments["user_question"]
		if !ok || userQuestion == "" {
			return nil, fmt.Errorf("user_question parameter is required")
		}

		messages := []mcp.PromptMessage{
			{
				Role: "system",
				Content: mcp.TextContent{
					Type: "text",
					Text: `You have access to the gpt_websearch tool that performs web searches using OpenAI's GPT models. ` +
						`This tool searches the web, gathers sources, reads them, and provides a single comprehensive answer.

CRITICAL: You MUST use the gpt_websearch tool to answer the user's question. Do not rely on your training data alone.

## Model Selection (choose cost-effectively):
- gpt-5-nano: Simple facts, definitions, quick lookups, basic summaries
- gpt-5-mini: Well-defined research tasks, comparisons, specific topics with clear scope  
- gpt-5: Complex analysis, coding questions, multi-faceted problems, reasoning tasks

## Reasoning Effort Selection:
- low: Factual queries, simple definitions, straightforward questions (3 min timeout)
- medium: Research requiring synthesis, comparisons, moderate complexity (5 min timeout)  
- high: Complex analysis, multi-part questions, deep research (10 min timeout)

## Search Strategy:
1. ANALYZE the user's question in the context of our conversation
2. FORMULATE detailed, specific search queries (expand beyond the original question with context and specifics)
3. DECIDE on search approach:
   - Single comprehensive search: When question can be fully addressed in one query
   - Sequential searches: When answers build on each other or need follow-up
   - Parallel searches: When covering different aspects of the same topic
4. SELECT appropriate model and reasoning_effort for each search
5. SYNTHESIZE results into a comprehensive, coherent answer

## Query Formulation Guidelines:
- Expand user questions with conversation context and specifics
- Include relevant constraints (timeframe, geographic scope, domain)
- Make queries specific enough to get focused, useful results
- Consider breaking complex questions into focused sub-queries

## Important Notes:
- The tool returns comprehensive answers, not citations or links to extract
- Be cost-conscious: use the simplest model that can handle the complexity
- You may need multiple searches for comprehensive coverage
- Always address the original user question completely

Now use the gpt_websearch tool strategically to answer the user's question.`,
				},
			},
			{
				Role: "user",
				Content: mcp.TextContent{
					Type: "text",
					Text: userQuestion,
				},
			},
		}

		return &mcp.GetPromptResult{
			Messages: messages,
		}, nil
	})

	return mcpServer
}
</file>

<file path="go.mod">
module Answer

go 1.25.0

require (
	github.com/joho/godotenv v1.5.1
	github.com/mark3labs/mcp-go v0.37.0
)

require (
	github.com/bahlo/generic-list-go v0.2.0 // indirect
	github.com/buger/jsonparser v1.1.1 // indirect
	github.com/google/uuid v1.6.0 // indirect
	github.com/invopop/jsonschema v0.13.0 // indirect
	github.com/mailru/easyjson v0.7.7 // indirect
	github.com/spf13/cast v1.7.1 // indirect
	github.com/wk8/go-ordered-map/v2 v2.1.8 // indirect
	github.com/yosida95/uritemplate/v3 v3.0.2 // indirect
	gopkg.in/yaml.v3 v3.0.1 // indirect
)
</file>

<file path="main.go">
package main

import (
	"context"
	"encoding/json"
	"flag"
	"fmt"
	"os"
	"time"

	_ "github.com/joho/godotenv/autoload"
)

func main() {
	// Check if this is MCP server mode
	if len(os.Args) > 1 && os.Args[1] == "mcp" {
		RunMCPServer()
		return
	}

	// Original CLI mode
	runCLI()
}

func runCLI() {
	// Load environment-backed configuration
	envCfg, err := loadEnvConfig()
	if err != nil {
		fail(2, err.Error())
	}

	// Parse CLI flags
	var (
		baseURL = flag.String("base", defaultBaseURL, "API endpoint")
		model   = flag.String("model", func() string {
			if envCfg.Model != "" {
				return envCfg.Model
			}
			return defaultModel
		}(), "model (env MODEL)")
		effort = flag.String("effort", func() string {
			if envCfg.Effort != "" {
				return envCfg.Effort
			}
			return defaultEffort
		}(), "effort (env EFFORT)")
		questionVal string
		timeout     = flag.Duration("timeout", func() time.Duration {
			if envCfg.HasTimeout {
				return envCfg.Timeout
			}
			return getTimeoutForEffort(*effort)
		}(), "HTTP timeout (env TIMEOUT)")
		showAll = flag.Bool("show-all", func() bool {
			if envCfg.HasShowAll {
				return envCfg.ShowAll
			}
			return false
		}(), "print raw JSON response (env SHOW_ALL)")
	)
	flag.StringVar(&questionVal, "q", envCfg.Question, "question prompt (env QUESTION)")
	flag.StringVar(&questionVal, "question", envCfg.Question, "same as -q (env QUESTION)")
	flag.Parse()

	// Determine the final question value
	q := questionVal
	var questionFlagSet bool
	flag.Visit(func(f *flag.Flag) {
		if f.Name == "q" || f.Name == "question" {
			questionFlagSet = true
		}
	})
	if !questionFlagSet {
		if flag.NArg() > 0 {
			q = flag.Arg(0)
		}
	}
	if q == "" {
		fail(2, "please provide a question to ask (use -q flag or positional argument)")
	}

	// If timeout wasn't explicitly set, use effort-based timeout
	if !envCfg.HasTimeout {
		flag.Visit(func(f *flag.Flag) {
			if f.Name == "timeout" {
				return // User set it explicitly
			}
		})
		*timeout = getTimeoutForEffort(*effort)
	}

	// Make API call
	ctx := context.Background()
	apiResp, err := CallAPI(ctx, envCfg.APIKey, *baseURL, q, *model, *effort, *timeout)
	if err != nil {
		fail(2, err.Error())
	}

	if *showAll {
		// Print the full raw JSON when requested
		raw, _ := json.MarshalIndent(apiResp, "", "  ") //nolint:errcheck // Debug output, error ok to ignore
		fmt.Println(string(raw))
		return
	}

	// Extract and print the answer
	answer := ExtractAnswer(apiResp)
	if answer == "" {
		fail(3, "no answer found in response")
	}
	fmt.Println(answer)
}
</file>

</files>
